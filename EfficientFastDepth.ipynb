{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastDepth_blocks_EfficientNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IQVs3pGOHlp5",
        "FPD4zxidQJsk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J2gTkcfUkuN",
        "outputId": "1e25b6b7-d9c6-420a-853e-f9f47aeb4aea"
      },
      "source": [
        "!pip install scipy==1.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.2.2 in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.2) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ArL2vDehGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696b7dab-c109-4800-c124-b401e62c8f87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubLnV1xUE9xR"
      },
      "source": [
        "# Data Loader Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1bVC1StFRzp"
      },
      "source": [
        "## Transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvSUPA5DFRI6"
      },
      "source": [
        "from __future__ import division\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "\n",
        "import numpy as np\n",
        "import numbers\n",
        "import types\n",
        "import collections\n",
        "import warnings\n",
        "\n",
        "import scipy.ndimage.interpolation as itpl\n",
        "import scipy.misc as misc\n",
        "\n",
        "\n",
        "def _is_numpy_image(img):\n",
        "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n",
        "\n",
        "def _is_pil_image(img):\n",
        "    if accimage is not None:\n",
        "        return isinstance(img, (Image.Image, accimage.Image))\n",
        "    else:\n",
        "        return isinstance(img, Image.Image)\n",
        "\n",
        "def _is_tensor_image(img):\n",
        "    return torch.is_tensor(img) and img.ndimension() == 3\n",
        "\n",
        "def adjust_brightness(img, brightness_factor):\n",
        "    \"\"\"Adjust brightness of an Image.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image): PIL Image to be adjusted.\n",
        "        brightness_factor (float):  How much to adjust the brightness. Can be\n",
        "            any non negative number. 0 gives a black image, 1 gives the\n",
        "            original image while 2 increases the brightness by a factor of 2.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image: Brightness adjusted image.\n",
        "    \"\"\"\n",
        "    if not _is_pil_image(img):\n",
        "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "    enhancer = ImageEnhance.Brightness(img)\n",
        "    img = enhancer.enhance(brightness_factor)\n",
        "    return img\n",
        "\n",
        "\n",
        "def adjust_contrast(img, contrast_factor):\n",
        "    \"\"\"Adjust contrast of an Image.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image): PIL Image to be adjusted.\n",
        "        contrast_factor (float): How much to adjust the contrast. Can be any\n",
        "            non negative number. 0 gives a solid gray image, 1 gives the\n",
        "            original image while 2 increases the contrast by a factor of 2.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image: Contrast adjusted image.\n",
        "    \"\"\"\n",
        "    if not _is_pil_image(img):\n",
        "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img = enhancer.enhance(contrast_factor)\n",
        "    return img\n",
        "\n",
        "\n",
        "def adjust_saturation(img, saturation_factor):\n",
        "    \"\"\"Adjust color saturation of an image.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image): PIL Image to be adjusted.\n",
        "        saturation_factor (float):  How much to adjust the saturation. 0 will\n",
        "            give a black and white image, 1 will give the original image while\n",
        "            2 will enhance the saturation by a factor of 2.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image: Saturation adjusted image.\n",
        "    \"\"\"\n",
        "    if not _is_pil_image(img):\n",
        "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "    enhancer = ImageEnhance.Color(img)\n",
        "    img = enhancer.enhance(saturation_factor)\n",
        "    return img\n",
        "\n",
        "\n",
        "def adjust_hue(img, hue_factor):\n",
        "    \"\"\"Adjust hue of an image.\n",
        "\n",
        "    The image hue is adjusted by converting the image to HSV and\n",
        "    cyclically shifting the intensities in the hue channel (H).\n",
        "    The image is then converted back to original image mode.\n",
        "\n",
        "    `hue_factor` is the amount of shift in H channel and must be in the\n",
        "    interval `[-0.5, 0.5]`.\n",
        "\n",
        "    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image): PIL Image to be adjusted.\n",
        "        hue_factor (float):  How much to shift the hue channel. Should be in\n",
        "            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n",
        "            HSV space in positive and negative direction respectively.\n",
        "            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n",
        "            with complementary colors while 0 gives the original image.\n",
        "\n",
        "    Returns:\n",
        "        PIL Image: Hue adjusted image.\n",
        "    \"\"\"\n",
        "    if not(-0.5 <= hue_factor <= 0.5):\n",
        "        raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n",
        "\n",
        "    if not _is_pil_image(img):\n",
        "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "    input_mode = img.mode\n",
        "    if input_mode in {'L', '1', 'I', 'F'}:\n",
        "        return img\n",
        "\n",
        "    h, s, v = img.convert('HSV').split()\n",
        "\n",
        "    np_h = np.array(h, dtype=np.uint8)\n",
        "    # uint8 addition take cares of rotation across boundaries\n",
        "    with np.errstate(over='ignore'):\n",
        "        np_h += np.uint8(hue_factor * 255)\n",
        "    h = Image.fromarray(np_h, 'L')\n",
        "\n",
        "    img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n",
        "    return img\n",
        "\n",
        "\n",
        "def adjust_gamma(img, gamma, gain=1):\n",
        "    \"\"\"Perform gamma correction on an image.\n",
        "\n",
        "    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n",
        "    based on the following equation:\n",
        "\n",
        "        I_out = 255 * gain * ((I_in / 255) ** gamma)\n",
        "\n",
        "    See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n",
        "\n",
        "    Args:\n",
        "        img (PIL Image): PIL Image to be adjusted.\n",
        "        gamma (float): Non negative real number. gamma larger than 1 make the\n",
        "            shadows darker, while gamma smaller than 1 make dark regions\n",
        "            lighter.\n",
        "        gain (float): The constant multiplier.\n",
        "    \"\"\"\n",
        "    if not _is_pil_image(img):\n",
        "        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "    if gamma < 0:\n",
        "        raise ValueError('Gamma should be a non-negative real number')\n",
        "\n",
        "    input_mode = img.mode\n",
        "    img = img.convert('RGB')\n",
        "\n",
        "    np_img = np.array(img, dtype=np.float32)\n",
        "    np_img = 255 * gain * ((np_img / 255) ** gamma)\n",
        "    np_img = np.uint8(np.clip(np_img, 0, 255))\n",
        "\n",
        "    img = Image.fromarray(np_img, 'RGB').convert(input_mode)\n",
        "    return img\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "\n",
        "    Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        for t in self.transforms:\n",
        "            img = t(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
        "\n",
        "    Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
        "\n",
        "        Args:\n",
        "            img (numpy.ndarray): Image to be converted to tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "\n",
        "        if isinstance(img, np.ndarray):\n",
        "            # handle numpy array\n",
        "            if img.ndim == 3:\n",
        "                img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n",
        "            elif img.ndim == 2:\n",
        "                img = torch.from_numpy(img.copy())\n",
        "            else:\n",
        "                raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n",
        "\n",
        "            # backward compatibility\n",
        "            # return img.float().div(255)\n",
        "            return img.float()\n",
        "\n",
        "\n",
        "class NormalizeNumpyArray(object):\n",
        "    \"\"\"Normalize a ``numpy.ndarray`` with mean and standard deviation.\n",
        "    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n",
        "    will normalize each channel of the input ``numpy.ndarray`` i.e.\n",
        "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
        "\n",
        "    Args:\n",
        "        mean (sequence): Sequence of means for each channel.\n",
        "        std (sequence): Sequence of standard deviations for each channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray): Image of size (H, W, C) to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "        # TODO: make efficient\n",
        "        print(img.shape)\n",
        "        for i in range(3):\n",
        "            img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i]\n",
        "        return img\n",
        "\n",
        "class NormalizeTensor(object):\n",
        "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
        "    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n",
        "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
        "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
        "\n",
        "    Args:\n",
        "        mean (sequence): Sequence of means for each channel.\n",
        "        std (sequence): Sequence of standard deviations for each channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Normalized Tensor image.\n",
        "        \"\"\"\n",
        "        if not _is_tensor_image(tensor):\n",
        "            raise TypeError('tensor is not a torch image.')\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.sub_(m).div_(s)\n",
        "        return tensor\n",
        "\n",
        "class Rotate(object):\n",
        "    \"\"\"Rotates the given ``numpy.ndarray``.\n",
        "\n",
        "    Args:\n",
        "        angle (float): The rotation angle in degrees.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be rotated.\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): Rotated image.\n",
        "        \"\"\"\n",
        "\n",
        "        # order=0 means nearest-neighbor type interpolation\n",
        "        return itpl.rotate(img, self.angle, reshape=False, prefilter=False, order=0)\n",
        "\n",
        "\n",
        "class Resize(object):\n",
        "    \"\"\"Resize the the given ``numpy.ndarray`` to the given size.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size. If size is a sequence like\n",
        "            (h, w), output size will be matched to this. If size is an int,\n",
        "            smaller edge of the image will be matched to this number.\n",
        "            i.e, if height > width, then image will be rescaled to\n",
        "            (size * height / width, size)\n",
        "        interpolation (int, optional): Desired interpolation. Default is\n",
        "            ``PIL.Image.BILINEAR``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation='nearest'):\n",
        "        assert isinstance(size, int) or isinstance(size, float) or \\\n",
        "               (isinstance(size, collections.Iterable) and len(size) == 2)\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be scaled.\n",
        "        Returns:\n",
        "            PIL Image: Rescaled image.\n",
        "        \"\"\"\n",
        "        if img.ndim == 3:\n",
        "            return misc.imresize(img, self.size, self.interpolation)\n",
        "        elif img.ndim == 2:\n",
        "            return misc.imresize(img, self.size, self.interpolation, 'F')\n",
        "        else:\n",
        "            RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    \"\"\"Crops the given ``numpy.ndarray`` at the center.\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img, output_size):\n",
        "        \"\"\"Get parameters for ``crop`` for center crop.\n",
        "\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be cropped.\n",
        "            output_size (tuple): Expected output size of the crop.\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for center crop.\n",
        "        \"\"\"\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        th, tw = output_size\n",
        "        i = int(round((h - th) / 2.))\n",
        "        j = int(round((w - tw) / 2.))\n",
        "\n",
        "        # # randomized cropping\n",
        "        # i = np.random.randint(i-3, i+4)\n",
        "        # j = np.random.randint(j-3, j+4)\n",
        "\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): Cropped image.\n",
        "        \"\"\"\n",
        "        i, j, h, w = self.get_params(img, self.size)\n",
        "\n",
        "        \"\"\"\n",
        "        i: Upper pixel coordinate.\n",
        "        j: Left pixel coordinate.\n",
        "        h: Height of the cropped image.\n",
        "        w: Width of the cropped image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "        if img.ndim == 3:\n",
        "            return img[i:i+h, j:j+w, :]\n",
        "        elif img.ndim == 2:\n",
        "            return img[i:i + h, j:j + w]\n",
        "        else:\n",
        "            raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n",
        "\n",
        "class BottomCrop(object):\n",
        "    \"\"\"Crops the given ``numpy.ndarray`` at the bottom.\n",
        "\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img, output_size):\n",
        "        \"\"\"Get parameters for ``crop`` for bottom crop.\n",
        "\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be cropped.\n",
        "            output_size (tuple): Expected output size of the crop.\n",
        "\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for bottom crop.\n",
        "        \"\"\"\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        th, tw = output_size\n",
        "        i = h - th\n",
        "        j = int(round((w - tw) / 2.))\n",
        "\n",
        "        # randomized left and right cropping\n",
        "        # i = np.random.randint(i-3, i+4)\n",
        "        # j = np.random.randint(j-1, j+1)\n",
        "\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be cropped.\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): Cropped image.\n",
        "        \"\"\"\n",
        "        i, j, h, w = self.get_params(img, self.size)\n",
        "\n",
        "        \"\"\"\n",
        "        i: Upper pixel coordinate.\n",
        "        j: Left pixel coordinate.\n",
        "        h: Height of the cropped image.\n",
        "        w: Width of the cropped image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "        if img.ndim == 3:\n",
        "            return img[i:i+h, j:j+w, :]\n",
        "        elif img.ndim == 2:\n",
        "            return img[i:i + h, j:j + w]\n",
        "        else:\n",
        "            raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n",
        "\n",
        "class Lambda(object):\n",
        "    \"\"\"Apply a user-defined lambda as a transform.\n",
        "\n",
        "    Args:\n",
        "        lambd (function): Lambda/function to be used for transform.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        assert isinstance(lambd, types.LambdaType)\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.lambd(img)\n",
        "\n",
        "\n",
        "class HorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given ``numpy.ndarray``.\n",
        "\n",
        "    Args:\n",
        "        do_flip (boolean): whether or not do horizontal flip.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, do_flip):\n",
        "        self.do_flip = do_flip\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be flipped.\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): flipped image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "\n",
        "        if self.do_flip:\n",
        "            return np.fliplr(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "class ColorJitter(object):\n",
        "    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n",
        "\n",
        "    Args:\n",
        "        brightness (float): How much to jitter brightness. brightness_factor\n",
        "            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n",
        "        contrast (float): How much to jitter contrast. contrast_factor\n",
        "            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n",
        "        saturation (float): How much to jitter saturation. saturation_factor\n",
        "            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n",
        "        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n",
        "            [-hue, hue]. Should be >=0 and <= 0.5.\n",
        "    \"\"\"\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "        self.saturation = saturation\n",
        "        self.hue = hue\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(brightness, contrast, saturation, hue):\n",
        "        \"\"\"Get a randomized transform to be applied on image.\n",
        "\n",
        "        Arguments are same as that of __init__.\n",
        "\n",
        "        Returns:\n",
        "            Transform which randomly adjusts brightness, contrast and\n",
        "            saturation in a random order.\n",
        "        \"\"\"\n",
        "        transforms = []\n",
        "        if brightness > 0:\n",
        "            brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness)\n",
        "            transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n",
        "\n",
        "        if contrast > 0:\n",
        "            contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast)\n",
        "            transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n",
        "\n",
        "        if saturation > 0:\n",
        "            saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation)\n",
        "            transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor)))\n",
        "\n",
        "        if hue > 0:\n",
        "            hue_factor = np.random.uniform(-hue, hue)\n",
        "            transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor)))\n",
        "\n",
        "        np.random.shuffle(transforms)\n",
        "        transform = Compose(transforms)\n",
        "\n",
        "        return transform\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Input image.\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): Color jittered image.\n",
        "        \"\"\"\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "\n",
        "        pil = Image.fromarray(img)\n",
        "        transform = self.get_params(self.brightness, self.contrast,\n",
        "                                    self.saturation, self.hue)\n",
        "        return np.array(transform(pil))\n",
        "\n",
        "class Crop(object):\n",
        "    \"\"\"Crops the given PIL Image to a rectangular region based on a given\n",
        "    4-tuple defining the left, upper pixel coordinated, hight and width size.\n",
        "\n",
        "    Args:\n",
        "        a tuple: (upper pixel coordinate, left pixel coordinate, hight, width)-tuple\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, i, j, h, w):\n",
        "        \"\"\"\n",
        "        i: Upper pixel coordinate.\n",
        "        j: Left pixel coordinate.\n",
        "        h: Height of the cropped image.\n",
        "        w: Width of the cropped image.\n",
        "        \"\"\"\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (numpy.ndarray (C x H x W)): Image to be cropped.\n",
        "        Returns:\n",
        "            img (numpy.ndarray (C x H x W)): Cropped image.\n",
        "        \"\"\"\n",
        "\n",
        "        i, j, h, w = self.i, self.j, self.h, self.w\n",
        "\n",
        "        if not(_is_numpy_image(img)):\n",
        "            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n",
        "        if img.ndim == 3:\n",
        "            return img[i:i + h, j:j + w, :]\n",
        "        elif img.ndim == 2:\n",
        "            return img[i:i + h, j:j + w]\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                'img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(i={0},j={1},h={2},w={3})'.format(\n",
        "            self.i, self.j, self.h, self.w)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiIlg_iHFbAe"
      },
      "source": [
        "## My DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeWnFlUE9T2"
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import h5py\n",
        "\n",
        "\n",
        "def h5_loader(path):\n",
        "    h5f = h5py.File(path, \"r\")\n",
        "    rgb = np.array(h5f['rgb'])\n",
        "    rgb = np.transpose(rgb, (1, 2, 0))\n",
        "    depth = np.array(h5f['depth'])\n",
        "    return rgb, depth\n",
        "\n",
        "# def rgb2grayscale(rgb):\n",
        "#     return rgb[:,:,0] * 0.2989 + rgb[:,:,1] * 0.587 + rgb[:,:,2] * 0.114\n",
        "\n",
        "class MyDataloader(data.Dataset):\n",
        "    modality_names = ['rgb']\n",
        "\n",
        "    def is_image_file(self, filename):\n",
        "        IMG_EXTENSIONS = ['.h5']\n",
        "        return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "    def find_classes(self, dir):\n",
        "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "        classes.sort()\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        "\n",
        "    def make_dataset(self, dir, class_to_idx):\n",
        "  \n",
        "        images = []\n",
        "        dir = os.path.expanduser(dir)\n",
        "        for target in sorted(os.listdir(dir)):\n",
        "            d = os.path.join(dir, target)\n",
        "            if not os.path.isdir(d):\n",
        "                continue\n",
        "            for root, _, fnames in sorted(os.walk(d)):\n",
        "                for fname in sorted(fnames):\n",
        "                    if self.is_image_file(fname):\n",
        "                        path = os.path.join(root, fname)\n",
        "                        item = (path, class_to_idx[target])\n",
        "                        images.append(item)\n",
        "        return images\n",
        "\n",
        "    color_jitter = ColorJitter(0.4, 0.4, 0.4)\n",
        "\n",
        "    def __init__(self, root, split, modality='rgb', loader=h5_loader):\n",
        "        classes, class_to_idx = self.find_classes(root)\n",
        "        imgs = self.make_dataset(root, class_to_idx)\n",
        "        assert len(imgs)>0, \"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
        "        print(\"Found {} images in {} folder.\".format(len(imgs), split))\n",
        "        self.root = root\n",
        "        self.imgs = imgs\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        if split == 'train':\n",
        "            self.transform = self.train_transform\n",
        "        elif split == 'holdout':\n",
        "            self.transform = self.val_transform\n",
        "        elif split == 'val':\n",
        "            self.transform = self.val_transform\n",
        "        else:\n",
        "            raise (RuntimeError(\"Invalid dataset split: \" + split + \"\\n\"\n",
        "                                \"Supported dataset splits are: train, val\"))\n",
        "        self.loader = loader\n",
        "\n",
        "        assert (modality in self.modality_names), \"Invalid modality split: \" + modality + \"\\n\" + \\\n",
        "                                \"Supported dataset splits are: \" + ''.join(self.modality_names)\n",
        "        self.modality = modality\n",
        "\n",
        "    def train_transform(self, rgb, depth):\n",
        "        raise (RuntimeError(\"train_transform() is not implemented. \"))\n",
        "\n",
        "    def val_transform(rgb, depth):\n",
        "        raise (RuntimeError(\"val_transform() is not implemented.\"))\n",
        "\n",
        "    def __getraw__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (rgb, depth) the raw data.\n",
        "        \"\"\"\n",
        "        path, target = self.imgs[index]\n",
        "        rgb, depth = self.loader(path)\n",
        "        return rgb, depth\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rgb, depth = self.__getraw__(index)\n",
        "        if self.transform is not None:\n",
        "            rgb_np, depth_np = self.transform(rgb, depth)\n",
        "        else:\n",
        "            raise(RuntimeError(\"transform not defined\"))\n",
        "\n",
        "        # color normalization\n",
        "        # rgb_tensor = normalize_rgb(rgb_tensor)\n",
        "        # rgb_np = normalize_np(rgb_np)\n",
        "\n",
        "        if self.modality == 'rgb':\n",
        "            input_np = rgb_np\n",
        "\n",
        "        to_tensor = ToTensor()\n",
        "        input_tensor = to_tensor(input_np)\n",
        "        while input_tensor.dim() < 3:\n",
        "            input_tensor = input_tensor.unsqueeze(0)\n",
        "        depth_tensor = to_tensor(depth_np)\n",
        "        depth_tensor = depth_tensor.unsqueeze(0)\n",
        "\n",
        "        return input_tensor, depth_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qp4VwVhFgpd"
      },
      "source": [
        "## NYU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4tXjxYEFioc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "iheight, iwidth = 480, 640 # raw image size\n",
        "\n",
        "class NYUDataset(MyDataloader):\n",
        "    def __init__(self, root, split, modality='rgb'):\n",
        "        self.split = split\n",
        "        super(NYUDataset, self).__init__(root, split, modality)\n",
        "        self.output_size = (224, 224)\n",
        "\n",
        "    def is_image_file(self, filename):\n",
        "        # IMG_EXTENSIONS = ['.h5']\n",
        "        if self.split == 'train':\n",
        "            return (filename.endswith('.h5') and \\\n",
        "                '00001.h5' not in filename and '00201.h5' not in filename)\n",
        "        elif self.split == 'holdout':\n",
        "            return ('00001.h5' in filename or '00201.h5' in filename)\n",
        "        elif self.split == 'val':\n",
        "            return (filename.endswith('.h5'))\n",
        "        else:\n",
        "            raise (RuntimeError(\"Invalid dataset split: \" + split + \"\\n\"\n",
        "                                \"Supported dataset splits are: train, val\"))\n",
        "\n",
        "    def train_transform(self, rgb, depth):\n",
        "        s = np.random.uniform(1.0, 1.5) # random scaling\n",
        "        depth_np = depth / s\n",
        "        angle = np.random.uniform(-5.0, 5.0) # random rotation degrees\n",
        "        do_flip = np.random.uniform(0.0, 1.0) < 0.5 # random horizontal flip\n",
        "\n",
        "        # perform 1st step of data augmentation\n",
        "        transform = Compose([\n",
        "            Resize(250.0 / iheight), # this is for computational efficiency, since rotation can be slow\n",
        "            Rotate(angle),\n",
        "            Resize(s),\n",
        "            CenterCrop((228, 304)),\n",
        "            HorizontalFlip(do_flip),\n",
        "            Resize(self.output_size),\n",
        "        ])\n",
        "        rgb_np = transform(rgb)\n",
        "        rgb_np = self.color_jitter(rgb_np) # random color jittering\n",
        "        rgb_np = np.asfarray(rgb_np, dtype='float') / 255\n",
        "        depth_np = transform(depth_np)\n",
        "\n",
        "        return rgb_np, depth_np\n",
        "\n",
        "    def val_transform(self, rgb, depth):\n",
        "        depth_np = depth\n",
        "        transform = Compose([\n",
        "            Resize(250.0 / iheight),\n",
        "            CenterCrop((228, 304)),\n",
        "            Resize(self.output_size),\n",
        "        ])\n",
        "        rgb_np = transform(rgb)\n",
        "        rgb_np = np.asfarray(rgb_np, dtype='float') / 255\n",
        "        depth_np = transform(depth_np)\n",
        "\n",
        "        return rgb_np, depth_np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF-IeWs2GB1O"
      },
      "source": [
        "# Main Code "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXiI0ULrGH6j"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RmariC5GFZu"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "\n",
        "def parse_command():\n",
        "    data_names = ['nyudepthv2']\n",
        "    model_names = ['resnet18', 'resnet50']\n",
        "    loss_names = ['l1', 'l2']\n",
        "\n",
        "    \n",
        "    modality_names = MyDataloader.modality_names\n",
        "    \n",
        "    decoder_names = Decoder.names\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='FastDepth')\n",
        "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18', choices=model_names,\n",
        "                        help='model architecture: ' + ' | '.join(model_names) + ' (default: resnet18)')\n",
        "    parser.add_argument('--data', metavar='DATA', default='nyudepthv2',\n",
        "                        choices=data_names,\n",
        "                        help='dataset: ' + ' | '.join(data_names) + ' (default: nyudepthv2)')\n",
        "    parser.add_argument('--decoder', '-d', metavar='DECODER', default='deconv2', choices=decoder_names,\n",
        "                        help='decoder: ' + ' | '.join(decoder_names) + ' (default: deconv2)')\n",
        "    \n",
        "    parser.add_argument('--epochs', default=15, type=int, metavar='N',\n",
        "                        help='number of total epochs to run (default: 15)')\n",
        "    parser.add_argument('-c', '--criterion', metavar='LOSS', default='l1', choices=loss_names,\n",
        "                        help='loss function: ' + ' | '.join(loss_names) + ' (default: l1)')\n",
        "    parser.add_argument('-b', '--batch-size', default=8, type=int, help='mini-batch size (default: 8)')\n",
        "    parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
        "                        metavar='LR', help='initial learning rate (default 0.01)')\n",
        "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                        help='momentum')\n",
        "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
        "                        metavar='W', help='weight decay (default: 1e-4)')\n",
        "\n",
        "    parser.add_argument('-s', '--num-samples', default=0, type=int, metavar='N',\n",
        "                        help='number of sparse depth samples (default: 0)')\n",
        "    parser.add_argument('--modality', '-m', metavar='MODALITY', default='rgb', choices=modality_names,\n",
        "                        help='modality: ' + ' | '.join(modality_names) + ' (default: rgb)')\n",
        "    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 16)')\n",
        "    parser.add_argument('--print-freq', '-p', default=50, type=int,\n",
        "                        metavar='N', help='print frequency (default: 50)')\n",
        "    parser.add_argument('-e', '--evaluate', default='', type=str, metavar='PATH',)\n",
        "    parser.add_argument('-t', '--train', default='', type=str,metavar='PATH',)\n",
        "\n",
        "    parser.add_argument('--gpu', default='0', type=str, metavar='N', help=\"gpu id\")\n",
        "    parser.set_defaults(cuda=True)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def colored_depthmap(depth, d_min=None, d_max=None):\n",
        "    if d_min is None:\n",
        "        d_min = np.min(depth)\n",
        "    if d_max is None:\n",
        "        d_max = np.max(depth)\n",
        "    depth_relative = (depth - d_min) / (d_max - d_min)\n",
        "    return 255 * cmap(depth_relative)[:,:,:3] # H, W, C\n",
        "\n",
        "\n",
        "def merge_into_row(input, depth_target, depth_pred):\n",
        "    rgb = 255 * np.transpose(np.squeeze(input.cpu().numpy()), (1,2,0)) # H, W, C\n",
        "    depth_target_cpu = np.squeeze(depth_target.cpu().numpy())\n",
        "    depth_pred_cpu = np.squeeze(depth_pred.data.cpu().numpy())\n",
        "\n",
        "    d_min = min(np.min(depth_target_cpu), np.min(depth_pred_cpu))\n",
        "    d_max = max(np.max(depth_target_cpu), np.max(depth_pred_cpu))\n",
        "    depth_target_col = colored_depthmap(depth_target_cpu, d_min, d_max)\n",
        "    depth_pred_col = colored_depthmap(depth_pred_cpu, d_min, d_max)\n",
        "    img_merge = np.hstack([rgb, depth_target_col, depth_pred_col])\n",
        "    \n",
        "    return img_merge\n",
        "\n",
        "\n",
        "def merge_into_row_with_gt(input, depth_input, depth_target, depth_pred):\n",
        "    rgb = 255 * np.transpose(np.squeeze(input.cpu().numpy()), (1,2,0)) # H, W, C\n",
        "    depth_input_cpu = np.squeeze(depth_input.cpu().numpy())\n",
        "    depth_target_cpu = np.squeeze(depth_target.cpu().numpy())\n",
        "    depth_pred_cpu = np.squeeze(depth_pred.data.cpu().numpy())\n",
        "\n",
        "    d_min = min(np.min(depth_input_cpu), np.min(depth_target_cpu), np.min(depth_pred_cpu))\n",
        "    d_max = max(np.max(depth_input_cpu), np.max(depth_target_cpu), np.max(depth_pred_cpu))\n",
        "    depth_input_col = colored_depthmap(depth_input_cpu, d_min, d_max)\n",
        "    depth_target_col = colored_depthmap(depth_target_cpu, d_min, d_max)\n",
        "    depth_pred_col = colored_depthmap(depth_pred_cpu, d_min, d_max)\n",
        "\n",
        "    img_merge = np.hstack([rgb, depth_input_col, depth_target_col, depth_pred_col])\n",
        "\n",
        "    return img_merge\n",
        "\n",
        "\n",
        "def add_row(img_merge, row):\n",
        "    return np.vstack([img_merge, row])\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr_init):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 5 epochs\"\"\"\n",
        "    lr = lr_init * (0.1 ** (epoch // 5))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        \n",
        "def get_output_directory(args):\n",
        "    output_directory = os.path.join('results',\n",
        "        '{}.sparsifier={}.samples={}.modality={}.arch={}.decoder={}.criterion={}.lr={}.bs={}.pretrained={}'.\n",
        "        format(args.data, None, args.num_samples, args.modality, \\\n",
        "            args.arch, args.decoder, args.criterion, args.lr, args.batch_size, \\\n",
        "            False))\n",
        "    return output_directory\n",
        "\n",
        "def save_checkpoint(state, is_best, epoch, output_directory):\n",
        "    checkpoint_filename = os.path.join(output_directory, 'checkpoint-' + str(epoch) + '.pth.tar')\n",
        "    torch.save(state, checkpoint_filename)\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(output_directory, 'model_best.pth.tar')\n",
        "        shutil.copyfile(checkpoint_filename, best_filename)\n",
        "    if epoch > 0:\n",
        "        prev_checkpoint_filename = os.path.join(output_directory, 'checkpoint-' + str(epoch-1) + '.pth.tar')\n",
        "        if os.path.exists(prev_checkpoint_filename):\n",
        "            os.remove(prev_checkpoint_filename)\n",
        "\n",
        "    checkpoint_filename1 = os.path.join(output_directory1, 'checkpoint-' + str(epoch) + '.pth.tar')\n",
        "    torch.save(state, checkpoint_filename1)\n",
        "    if is_best:\n",
        "        best_filename1 = os.path.join(output_directory1, 'model_best.pth.tar')\n",
        "        shutil.copyfile(checkpoint_filename1, best_filename1)\n",
        "    if epoch > 0:\n",
        "        prev_checkpoint_filename1 = os.path.join(output_directory1, 'checkpoint-' + str(epoch-1) + '.pth.tar')\n",
        "        if os.path.exists(prev_checkpoint_filename1):\n",
        "            os.remove(prev_checkpoint_filename1)\n",
        "\n",
        "\n",
        "def save_image(img_merge, filename):\n",
        "    img_merge = Image.fromarray(img_merge.astype('uint8'))\n",
        "    img_merge.save(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vxFEXpbGmjT"
      },
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H53qAbRBHfsY"
      },
      "source": [
        "### Mobilenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BirJIUtHfKi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "class MobileNet1(nn.Module):\n",
        "    def __init__(self, relu6=True):\n",
        "        super(MobileNet1, self).__init__()\n",
        "\n",
        "        def relu(relu6):\n",
        "            if relu6:\n",
        "                return nn.ReLU6(inplace=True)\n",
        "            else:\n",
        "                return nn.ReLU(inplace=True)\n",
        "\n",
        "        def conv_bn(inp, oup, stride, relu6):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "                relu(relu6),\n",
        "            )\n",
        "\n",
        "        def conv_dw(inp, oup, stride, relu6):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "                nn.BatchNorm2d(inp),\n",
        "                relu(relu6),\n",
        "    \n",
        "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "                relu(relu6),\n",
        "            )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            conv_bn(  3,  32, 2, relu6), \n",
        "            conv_dw( 32,  64, 1, relu6),\n",
        "            conv_dw( 64, 128, 2, relu6),\n",
        "            conv_dw(128, 128, 1, relu6),\n",
        "            conv_dw(128, 256, 2, relu6),\n",
        "            conv_dw(256, 256, 1, relu6),\n",
        "            conv_dw(256, 512, 2, relu6),\n",
        "            conv_dw(512, 512, 1, relu6),\n",
        "            conv_dw(512, 512, 1, relu6),\n",
        "            conv_dw(512, 512, 1, relu6),\n",
        "            conv_dw(512, 512, 1, relu6),\n",
        "            conv_dw(512, 512, 1, relu6),\n",
        "            conv_dw(512, 1024, 2, relu6),\n",
        "            conv_dw(1024, 1024, 1, relu6),\n",
        "            nn.AvgPool2d(7),\n",
        "        )\n",
        "        self.fc = nn.Linear(1024, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(-1, 1024)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDlEC70SWyK"
      },
      "source": [
        "### Efficient Net Modified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyVb9K4VSUR5"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.functional as F\n",
        "\n",
        "\n",
        "efficientnet_lite_params = {\n",
        "    # width_coefficient, depth_coefficient, image_size, dropout_rate\n",
        "    'efficientnet_lite0': [1.0, 1.0, 224, 0.2],\n",
        "    'efficientnet_lite1': [1.0, 1.1, 240, 0.2],\n",
        "    'efficientnet_lite2': [1.1, 1.2, 260, 0.3],\n",
        "    'efficientnet_lite3': [1.2, 1.4, 280, 0.3],\n",
        "    'efficientnet_lite4': [1.4, 1.8, 300, 0.3],\n",
        "}\n",
        "\n",
        "\n",
        "def round_filters(filters, multiplier, divisor=8, min_width=None):\n",
        "    \"\"\"Calculate and round number of filters based on width multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    filters *= multiplier\n",
        "    min_width = min_width or divisor\n",
        "    new_filters = max(min_width, int(filters + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_filters < 0.9 * filters:\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "def round_repeats(repeats, multiplier):\n",
        "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "def drop_connect(x, drop_connect_rate, training):\n",
        "    if not training:\n",
        "        return x\n",
        "    keep_prob = 1.0 - drop_connect_rate\n",
        "    batch_size = x.shape[0]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    binary_mask = torch.floor(random_tensor)\n",
        "    x = (x / keep_prob) * binary_mask\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, inp, final_oup, k, s, expand_ratio, se_ratio, has_se=False):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self._momentum = 0.01\n",
        "        self._epsilon = 1e-3\n",
        "        self.input_filters = inp\n",
        "        self.output_filters = final_oup\n",
        "        self.stride = s\n",
        "        self.expand_ratio = expand_ratio\n",
        "        self.has_se = has_se\n",
        "        self.id_skip = True  # skip connection and drop connect\n",
        "\n",
        "        # Expansion phase\n",
        "        oup = inp * expand_ratio  # number of output channels\n",
        "        if expand_ratio != 1:\n",
        "            self._expand_conv = nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        self._depthwise_conv = nn.Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, padding=(k - 1) // 2, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(inp * se_ratio))\n",
        "            self._se_reduce = nn.Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = nn.Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        self._project_conv = nn.Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._momentum, eps=self._epsilon)\n",
        "        self._relu = nn.ReLU6(inplace=True)\n",
        "\n",
        "    def forward(self, x, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param x: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        identity = x\n",
        "        if self.expand_ratio != 1:\n",
        "            x = self._relu(self._bn0(self._expand_conv(x)))\n",
        "        x = self._relu(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(self._relu(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        if self.id_skip and self.stride == 1  and self.input_filters == self.output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, drop_connect_rate, training=self.training)\n",
        "            x += identity  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNetLite(nn.Module):\n",
        "    def __init__(self, widthi_multiplier, depth_multiplier, num_classes, drop_connect_rate, dropout_rate):\n",
        "        super(EfficientNetLite, self).__init__()\n",
        "\n",
        "        # Batch norm parameters\n",
        "        momentum = 0.01\n",
        "        epsilon = 1e-3\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "\n",
        "        mb_block_settings = [\n",
        "            #repeat|kernal_size|stride|expand|input|output|se_ratio\n",
        "                [1, 3,  1,  1,  32,  16,  0.25], # 112\n",
        "                [3, 3,  1,  6,  16,  32,  0.25], # 112\n",
        "                [4, 5,  1,  6,  32,  64,  0.25], # 112\n",
        "                [4, 3,  2,  6,  64, 128,  0.25], #  56\n",
        "                [3, 5,  2,  6, 128, 256,  0.25], #  28\n",
        "                [1, 5,  2,  1, 256, 512,  0.25], #  14 \n",
        "                [1, 3,  2,  1, 512, 512,  0.25] #    7\n",
        "            ]\n",
        "\n",
        "        self.x1_shape = [8, 64, 112, 112]\n",
        "        self.x2_shape = [8, 128, 56, 56]\n",
        "        self.x3_shape = [8, 256, 28, 28]\n",
        "\n",
        "\n",
        "        # Stem\n",
        "        out_channels = 32\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Build blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i, stage_setting in enumerate(mb_block_settings):\n",
        "            stage = nn.ModuleList([])\n",
        "            num_repeat, kernal_size, stride, expand_ratio, input_filters, output_filters, se_ratio = stage_setting\n",
        "            # Update block input and output filters based on width multiplier.\n",
        "            input_filters = input_filters if i == 0 else round_filters(input_filters, widthi_multiplier)\n",
        "            output_filters = round_filters(output_filters, widthi_multiplier)\n",
        "            num_repeat= num_repeat if i == 0 or i == len(mb_block_settings) - 1  else round_repeats(num_repeat, depth_multiplier)\n",
        "            \n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            if num_repeat > 1:\n",
        "                input_filters = output_filters\n",
        "                stride = 1\n",
        "            for _ in range(num_repeat - 1):\n",
        "                stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            \n",
        "            self.blocks.append(stage)\n",
        "\n",
        "        # Head\n",
        "        in_channels = round_filters(mb_block_settings[-1][5], widthi_multiplier)\n",
        "        out_channels = 1024 #1280\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.fc = torch.nn.Linear(out_channels, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(f'SIZE: {list(x.shape)} of type: {type(list(x.shape))}')\n",
        "        x = self.stem(x)\n",
        "        \n",
        "        idx = 0\n",
        "        for idx, stage in enumerate(self.blocks):\n",
        "            counter = 0\n",
        "            for block in stage:\n",
        "                # print(f'SIZE: {list(x.shape)} of type: {type(list(x.shape))}')\n",
        "                drop_connect_rate = self.drop_connect_rate\n",
        "                if drop_connect_rate:\n",
        "                    drop_connect_rate *= float(idx) / len(self.blocks)\n",
        "                x = block(x, drop_connect_rate)\n",
        "\n",
        "                # print(f'{idx}, {counter}: {x.size()}')\n",
        "# \n",
        "                # print(f'SIZE: {list(x.shape)} of type: {type(list(x.shape))}')\n",
        "                if list(x.shape)[1:] == self.x1_shape[1:] and counter==0:\n",
        "                    x1 = x\n",
        "                elif list(x.shape)[1:] == self.x2_shape[1:] and counter==0:\n",
        "                    x2 = x\n",
        "                elif list(x.shape)[1:] == self.x3_shape[1:] and counter==0:\n",
        "                    x3 = x\n",
        "                idx +=1\n",
        "\n",
        "                counter += 1\n",
        "        x = self.head(x)\n",
        "        # print('Enc head ', x.size())\n",
        "\n",
        "        return x, x1, x2, x3\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 1.0/float(n))\n",
        "                m.bias.data.zero_()\n",
        "    \n",
        "    def load_pretrain(self, path):\n",
        "        state_dict = torch.load(path)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "        \n",
        "\n",
        "def build_efficientnet_lite(name, num_classes):\n",
        "    width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_lite_params[name]\n",
        "    model = EfficientNetLite(width_coefficient, depth_coefficient, num_classes, 0.2, dropout_rate)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0NwhH-avoRu"
      },
      "source": [
        "### EfficientNet Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLj69crvn5H"
      },
      "source": [
        "# https://github.com/RangiLyu/EfficientNet-Lite/blob/main/efficientnet_lite.py\n",
        "\n",
        "class EfficientNetLiteOriginal(nn.Module):\n",
        "    def __init__(self, widthi_multiplier, depth_multiplier, num_classes, drop_connect_rate, dropout_rate):\n",
        "        super(EfficientNetLiteOriginal, self).__init__()\n",
        "\n",
        "        # Batch norm parameters\n",
        "        momentum = 0.01\n",
        "        epsilon = 1e-3\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "\n",
        "        mb_block_settings = [\n",
        "            #repeat|kernal_size|stride|expand|input|output|se_ratio\n",
        "                [1, 3, 1, 1, 32,  16,  0.25],\n",
        "                [2, 3, 2, 6, 16,  24,  0.25],\n",
        "                [2, 5, 2, 6, 24,  40,  0.25],\n",
        "                [3, 3, 2, 6, 40,  80,  0.25],\n",
        "                [3, 5, 1, 6, 80,  112, 0.25],\n",
        "                [4, 5, 2, 6, 112, 192, 0.25],\n",
        "                [1, 3, 1, 6, 192, 320, 0.25]\n",
        "            ]\n",
        "\n",
        "        # Stem\n",
        "        out_channels = 32\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Build blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i, stage_setting in enumerate(mb_block_settings):\n",
        "            stage = nn.ModuleList([])\n",
        "            num_repeat, kernal_size, stride, expand_ratio, input_filters, output_filters, se_ratio = stage_setting\n",
        "            # Update block input and output filters based on width multiplier.\n",
        "            input_filters = input_filters if i == 0 else round_filters(input_filters, widthi_multiplier)\n",
        "            output_filters = round_filters(output_filters, widthi_multiplier)\n",
        "            num_repeat= num_repeat if i == 0 or i == len(mb_block_settings) - 1  else round_repeats(num_repeat, depth_multiplier)\n",
        "            \n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            if num_repeat > 1:\n",
        "                input_filters = output_filters\n",
        "                stride = 1\n",
        "            for _ in range(num_repeat - 1):\n",
        "                stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            \n",
        "            self.blocks.append(stage)\n",
        "\n",
        "        # Head\n",
        "        in_channels = round_filters(mb_block_settings[-1][5], widthi_multiplier)\n",
        "        out_channels = 1024\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.fc = torch.nn.Linear(out_channels, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        idx = 0\n",
        "        for stage in self.blocks:\n",
        "            for block in stage:\n",
        "                drop_connect_rate = self.drop_connect_rate\n",
        "                if drop_connect_rate:\n",
        "                    drop_connect_rate *= float(idx) / len(self.blocks)\n",
        "                x = block(x, drop_connect_rate)\n",
        "                idx +=1\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 1.0/float(n))\n",
        "                m.bias.data.zero_()\n",
        "    \n",
        "    def load_pretrain(self, path):\n",
        "        state_dict = torch.load(path)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "        \n",
        "\n",
        "def build_efficientnet_lite_original(name, num_classes):\n",
        "    width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_lite_params[name]\n",
        "    model = EfficientNetLiteOriginal(width_coefficient, depth_coefficient, num_classes, 0.2, dropout_rate)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQVs3pGOHlp5"
      },
      "source": [
        "### All other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bgBoldVGUMd"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "import collections\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    # a dummy identity module\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Unpool(nn.Module):\n",
        "    # Unpool: 2*2 unpooling with zero padding\n",
        "    def __init__(self, stride=2):\n",
        "        super(Unpool, self).__init__()\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        # create kernel [1, 0; 0, 0]\n",
        "        self.mask = torch.zeros(1, 1, stride, stride)\n",
        "        self.mask[:,:,0,0] = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 4\n",
        "        num_channels = x.size(1)\n",
        "        return F.conv_transpose2d(x,\n",
        "            self.mask.detach().type_as(x).expand(num_channels, 1, -1, -1),\n",
        "            stride=self.stride, groups=num_channels)\n",
        "\n",
        "def weights_init(m):\n",
        "    # Initialize kernel weights with Gaussian distributions\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size):\n",
        "    padding = (kernel_size-1) // 2\n",
        "    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n",
        "    return nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def depthwise(in_channels, kernel_size):\n",
        "    padding = (kernel_size-1) // 2\n",
        "    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n",
        "    return nn.Sequential(\n",
        "          nn.Conv2d(in_channels,in_channels,kernel_size,stride=1,padding=padding,bias=False,groups=in_channels),\n",
        "          nn.BatchNorm2d(in_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def pointwise(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,1,1,0,bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def convt(in_channels, out_channels, kernel_size):\n",
        "    stride = 2\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    output_padding = kernel_size % 2\n",
        "    assert -2 - 2*padding + kernel_size + output_padding == 0, \"deconv parameters incorrect\"\n",
        "    return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels,out_channels,kernel_size,\n",
        "                stride,padding,output_padding,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def convt_dw(channels, kernel_size):\n",
        "    stride = 2\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    output_padding = kernel_size % 2\n",
        "    assert -2 - 2*padding + kernel_size + output_padding == 0, \"deconv parameters incorrect\"\n",
        "    return nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels,channels,kernel_size,\n",
        "                stride,padding,output_padding,bias=False,groups=channels),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def upconv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        Unpool(2),\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "class upproj(nn.Module):\n",
        "    # UpProj module has two branches, with a Unpool at the start and a ReLu at the end\n",
        "    #   upper branch: 5*5 conv -> batchnorm -> ReLU -> 3*3 conv -> batchnorm\n",
        "    #   bottom branch: 5*5 conv -> batchnorm\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(upproj, self).__init__()\n",
        "        self.unpool = Unpool(2)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unpool(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        return F.relu(x1 + x2)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    names = ['deconv{}{}'.format(i,dw) for i in range(3,10,2) for dw in ['', 'dw']]\n",
        "    names.append(\"upconv\")\n",
        "    names.append(\"upproj\")\n",
        "    for i in range(3,10,2):\n",
        "        for dw in ['', 'dw']:\n",
        "            names.append(\"nnconv{}{}\".format(i, dw))\n",
        "            names.append(\"blconv{}{}\".format(i, dw))\n",
        "            names.append(\"shuffle{}{}\".format(i, dw))\n",
        "\n",
        "class DeConv(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size, dw):\n",
        "        super(DeConv, self).__init__()\n",
        "        if dw:\n",
        "            self.convt1 = nn.Sequential(\n",
        "                convt_dw(1024, kernel_size),\n",
        "                pointwise(1024, 512))\n",
        "            self.convt2 = nn.Sequential(\n",
        "                convt_dw(512, kernel_size),\n",
        "                pointwise(512, 256))\n",
        "            self.convt3 = nn.Sequential(\n",
        "                convt_dw(256, kernel_size),\n",
        "                pointwise(256, 128))\n",
        "            self.convt4 = nn.Sequential(\n",
        "                convt_dw(128, kernel_size),\n",
        "                pointwise(128, 64))\n",
        "            self.convt5 = nn.Sequential(\n",
        "                convt_dw(64, kernel_size),\n",
        "                pointwise(64, 32))\n",
        "        else:\n",
        "            self.convt1 = convt(1024, 512, kernel_size)\n",
        "            self.convt2 = convt(512, 256, kernel_size)\n",
        "            self.convt3 = convt(256, 128, kernel_size)\n",
        "            self.convt4 = convt(128, 64, kernel_size)\n",
        "            self.convt5 = convt(64, 32, kernel_size)\n",
        "        self.convf = pointwise(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convt1(x)\n",
        "        x = self.convt2(x)\n",
        "        x = self.convt3(x)\n",
        "        x = self.convt4(x)\n",
        "        x = self.convt5(x)\n",
        "        x = self.convf(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(UpConv, self).__init__()\n",
        "        self.upconv1 = upconv(1024, 512)\n",
        "        self.upconv2 = upconv(512, 256)\n",
        "        self.upconv3 = upconv(256, 128)\n",
        "        self.upconv4 = upconv(128, 64)\n",
        "        self.upconv5 = upconv(64, 32)\n",
        "        self.convf = pointwise(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upconv1(x)\n",
        "        x = self.upconv2(x)\n",
        "        x = self.upconv3(x)\n",
        "        x = self.upconv4(x)\n",
        "        x = self.upconv5(x)\n",
        "        x = self.convf(x)\n",
        "        return x\n",
        "\n",
        "class UpProj(nn.Module):\n",
        "    # UpProj decoder consists of 4 upproj modules with decreasing number of channels and increasing feature map size\n",
        "\n",
        "    def __init__(self):\n",
        "        super(UpProj, self).__init__()\n",
        "        self.upproj1 = upproj(1024, 512)\n",
        "        self.upproj2 = upproj(512, 256)\n",
        "        self.upproj3 = upproj(256, 128)\n",
        "        self.upproj4 = upproj(128, 64)\n",
        "        self.upproj5 = upproj(64, 32)\n",
        "        self.convf = pointwise(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upproj1(x)\n",
        "        x = self.upproj2(x)\n",
        "        x = self.upproj3(x)\n",
        "        x = self.upproj4(x)\n",
        "        x = self.upproj5(x)\n",
        "        x = self.convf(x)\n",
        "        return x\n",
        "\n",
        "class NNConv(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size, dw):\n",
        "        super(NNConv, self).__init__()\n",
        "        if dw:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                depthwise(1024, kernel_size),\n",
        "                pointwise(1024, 512))\n",
        "            self.conv2 = nn.Sequential(\n",
        "                depthwise(512, kernel_size),\n",
        "                pointwise(512, 256))\n",
        "            self.conv3 = nn.Sequential(\n",
        "                depthwise(256, kernel_size),\n",
        "                pointwise(256, 128))\n",
        "            self.conv4 = nn.Sequential(\n",
        "                depthwise(128, kernel_size),\n",
        "                pointwise(128, 64))\n",
        "            self.conv5 = nn.Sequential(\n",
        "                depthwise(64, kernel_size),\n",
        "                pointwise(64, 32))\n",
        "            self.conv6 = pointwise(32, 1)\n",
        "        else:\n",
        "            self.conv1 = conv(1024, 512, kernel_size)\n",
        "            self.conv2 = conv(512, 256, kernel_size)\n",
        "            self.conv3 = conv(256, 128, kernel_size)\n",
        "            self.conv4 = conv(128, 64, kernel_size)\n",
        "            self.conv5 = conv(64, 32, kernel_size)\n",
        "            self.conv6 = pointwise(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        return x\n",
        "\n",
        "class BLConv(NNConv):\n",
        "\n",
        "    def __init__(self, kernel_size, dw):\n",
        "        super(BLConv, self).__init__(kernel_size, dw)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        return x\n",
        "\n",
        "class ShuffleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size, dw):\n",
        "        super(ShuffleConv, self).__init__()\n",
        "        if dw:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                depthwise(256, kernel_size),\n",
        "                pointwise(256, 256))\n",
        "            self.conv2 = nn.Sequential(\n",
        "                depthwise(64, kernel_size),\n",
        "                pointwise(64, 64))\n",
        "            self.conv3 = nn.Sequential(\n",
        "                depthwise(16, kernel_size),\n",
        "                pointwise(16, 16))\n",
        "            self.conv4 = nn.Sequential(\n",
        "                depthwise(4, kernel_size),\n",
        "                pointwise(4, 4))\n",
        "        else:\n",
        "            self.conv1 = conv(256, 256, kernel_size)\n",
        "            self.conv2 = conv(64, 64, kernel_size)\n",
        "            self.conv3 = conv(16, 16, kernel_size)\n",
        "            self.conv4 = conv(4, 4, kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pixel_shuffle(x, 2)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = F.pixel_shuffle(x, 2)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = F.pixel_shuffle(x, 2)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = F.pixel_shuffle(x, 2)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = F.pixel_shuffle(x, 2)\n",
        "        return x\n",
        "\n",
        "def choose_decoder(decoder):\n",
        "    depthwise = ('dw' in decoder)\n",
        "    if decoder[:6] == 'deconv':\n",
        "        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n",
        "        kernel_size = int(decoder[6])\n",
        "        model = DeConv(kernel_size, depthwise)\n",
        "    elif decoder == \"upproj\":\n",
        "        model = UpProj()\n",
        "    elif decoder == \"upconv\":\n",
        "        model = UpConv()\n",
        "    elif decoder[:7] == 'shuffle':\n",
        "        assert len(decoder)==8 or (len(decoder)==10 and 'dw' in decoder)\n",
        "        kernel_size = int(decoder[7])\n",
        "        model = ShuffleConv(kernel_size, depthwise)\n",
        "    elif decoder[:6] == 'nnconv':\n",
        "        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n",
        "        kernel_size = int(decoder[6])\n",
        "        model = NNConv(kernel_size, depthwise)\n",
        "    elif decoder[:6] == 'blconv':\n",
        "        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n",
        "        kernel_size = int(decoder[6])\n",
        "        model = BLConv(kernel_size, depthwise)\n",
        "    else:\n",
        "        assert False, \"invalid option for decoder: {}\".format(decoder)\n",
        "    model.apply(weights_init)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers, decoder, output_size, in_channels=3, pretrained=True):\n",
        "\n",
        "        if layers not in [18, 34, 50, 101, 152]:\n",
        "            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n",
        "        \n",
        "        super(ResNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n",
        "        if not pretrained:\n",
        "            pretrained_model.apply(weights_init)\n",
        "        \n",
        "        if in_channels == 3:\n",
        "            self.conv1 = pretrained_model._modules['conv1']\n",
        "            self.bn1 = pretrained_model._modules['bn1']\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            weights_init(self.conv1)\n",
        "            weights_init(self.bn1)\n",
        "        \n",
        "        self.relu = pretrained_model._modules['relu']\n",
        "        self.maxpool = pretrained_model._modules['maxpool']\n",
        "        self.layer1 = pretrained_model._modules['layer1']\n",
        "        self.layer2 = pretrained_model._modules['layer2']\n",
        "        self.layer3 = pretrained_model._modules['layer3']\n",
        "        self.layer4 = pretrained_model._modules['layer4']\n",
        "\n",
        "        # clear memory\n",
        "        del pretrained_model\n",
        "\n",
        "        # define number of intermediate channels\n",
        "        if layers <= 34:\n",
        "            num_channels = 512\n",
        "        elif layers >= 50:\n",
        "            num_channels = 2048\n",
        "        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n",
        "        weights_init(self.conv2)\n",
        "        self.decoder = choose_decoder(decoder)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # resnet\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # decoder\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MobileNet(nn.Module):\n",
        "    def __init__(self, decoder, output_size, in_channels=3, pretrained=True):\n",
        "\n",
        "        super(MobileNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        mobilenet = imagenet.mobilenet.MobileNet()\n",
        "        if pretrained:\n",
        "            pretrained_path = os.path.join('imagenet', 'results', 'imagenet.arch=mobilenet.lr=0.1.bs=256', 'model_best.pth.tar')\n",
        "            checkpoint = torch.load(pretrained_path)\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            from collections import OrderedDict\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:] # remove `module.`\n",
        "                new_state_dict[name] = v\n",
        "            mobilenet.load_state_dict(new_state_dict)\n",
        "        else:\n",
        "            mobilenet.apply(weights_init)\n",
        "\n",
        "        if in_channels == 3:\n",
        "            self.mobilenet = nn.Sequential(*(mobilenet.model[i] for i in range(14)))\n",
        "        else:\n",
        "            def conv_bn(inp, oup, stride):\n",
        "                return nn.Sequential(\n",
        "                    nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "                    nn.BatchNorm2d(oup),\n",
        "                    nn.ReLU6(inplace=True)\n",
        "                )\n",
        "\n",
        "            self.mobilenet = nn.Sequential(\n",
        "                conv_bn(in_channels,  32, 2),\n",
        "                *(mobilenet.model[i] for i in range(1,14))\n",
        "                )\n",
        "\n",
        "        self.decoder = choose_decoder(decoder)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mobilenet(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class ResNetSkipAdd(nn.Module):\n",
        "    def __init__(self, layers, output_size, in_channels=3, pretrained=True):\n",
        "\n",
        "        if layers not in [18, 34, 50, 101, 152]:\n",
        "            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n",
        "        \n",
        "        super(ResNetSkipAdd, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n",
        "        if not pretrained:\n",
        "            pretrained_model.apply(weights_init)\n",
        "        \n",
        "        if in_channels == 3:\n",
        "            self.conv1 = pretrained_model._modules['conv1']\n",
        "            self.bn1 = pretrained_model._modules['bn1']\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            weights_init(self.conv1)\n",
        "            weights_init(self.bn1)\n",
        "        \n",
        "        self.relu = pretrained_model._modules['relu']\n",
        "        self.maxpool = pretrained_model._modules['maxpool']\n",
        "        self.layer1 = pretrained_model._modules['layer1']\n",
        "        self.layer2 = pretrained_model._modules['layer2']\n",
        "        self.layer3 = pretrained_model._modules['layer3']\n",
        "        self.layer4 = pretrained_model._modules['layer4']\n",
        "\n",
        "        # clear memory\n",
        "        del pretrained_model\n",
        "\n",
        "        # define number of intermediate channels\n",
        "        if layers <= 34:\n",
        "            num_channels = 512\n",
        "        elif layers >= 50:\n",
        "            num_channels = 2048\n",
        "        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n",
        "        weights_init(self.conv2)\n",
        "        \n",
        "        kernel_size = 5\n",
        "        self.decode_conv1 = conv(1024, 512, kernel_size)\n",
        "        self.decode_conv2 = conv(512, 256, kernel_size)\n",
        "        self.decode_conv3 = conv(256, 128, kernel_size)\n",
        "        self.decode_conv4 = conv(128, 64, kernel_size)\n",
        "        self.decode_conv5 = conv(64, 32, kernel_size)\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # resnet\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x1 = self.relu(x)\n",
        "        # print(\"x1\", x1.size())\n",
        "        x2 = self.maxpool(x1)\n",
        "        # print(\"x2\", x2.size())\n",
        "        x3 = self.layer1(x2)\n",
        "        # print(\"x3\", x3.size())\n",
        "        x4 = self.layer2(x3)\n",
        "        # print(\"x4\", x4.size())\n",
        "        x5 = self.layer3(x4)\n",
        "        # print(\"x5\", x5.size())\n",
        "        x6 = self.layer4(x5)\n",
        "        # print(\"x6\", x6.size())\n",
        "        x7 = self.conv2(x6)\n",
        "\n",
        "        # decoder\n",
        "        y10 = self.decode_conv1(x7)\n",
        "        # print(\"y10\", y10.size())\n",
        "        y9 = F.interpolate(y10 + x6, scale_factor=2, mode='nearest')\n",
        "        # print(\"y9\", y9.size())\n",
        "        y8 = self.decode_conv2(y9)\n",
        "        # print(\"y8\", y8.size())\n",
        "        y7 = F.interpolate(y8 + x5, scale_factor=2, mode='nearest')\n",
        "        # print(\"y7\", y7.size())\n",
        "        y6 = self.decode_conv3(y7)\n",
        "        # print(\"y6\", y6.size())\n",
        "        y5 = F.interpolate(y6 + x4, scale_factor=2, mode='nearest')\n",
        "        # print(\"y5\", y5.size())\n",
        "        y4 = self.decode_conv4(y5)\n",
        "        # print(\"y4\", y4.size())\n",
        "        y3 = F.interpolate(y4 + x3, scale_factor=2, mode='nearest')\n",
        "        # print(\"y3\", y3.size())\n",
        "        y2 = self.decode_conv5(y3 + x1)\n",
        "        # print(\"y2\", y2.size())\n",
        "        y1 = F.interpolate(y2, scale_factor=2, mode='nearest')\n",
        "        # print(\"y1\", y1.size())\n",
        "        y = self.decode_conv6(y1)\n",
        "\n",
        "        return y\n",
        "\n",
        "class ResNetSkipConcat(nn.Module):\n",
        "    def __init__(self, layers, output_size, in_channels=3, pretrained=True):\n",
        "\n",
        "        if layers not in [18, 34, 50, 101, 152]:\n",
        "            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n",
        "        \n",
        "        super(ResNetSkipConcat, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n",
        "        if not pretrained:\n",
        "            pretrained_model.apply(weights_init)\n",
        "        \n",
        "        if in_channels == 3:\n",
        "            self.conv1 = pretrained_model._modules['conv1']\n",
        "            self.bn1 = pretrained_model._modules['bn1']\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            weights_init(self.conv1)\n",
        "            weights_init(self.bn1)\n",
        "        \n",
        "        self.relu = pretrained_model._modules['relu']\n",
        "        self.maxpool = pretrained_model._modules['maxpool']\n",
        "        self.layer1 = pretrained_model._modules['layer1']\n",
        "        self.layer2 = pretrained_model._modules['layer2']\n",
        "        self.layer3 = pretrained_model._modules['layer3']\n",
        "        self.layer4 = pretrained_model._modules['layer4']\n",
        "\n",
        "        # clear memory\n",
        "        del pretrained_model\n",
        "\n",
        "        # define number of intermediate channels\n",
        "        if layers <= 34:\n",
        "            num_channels = 512\n",
        "        elif layers >= 50:\n",
        "            num_channels = 2048\n",
        "        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n",
        "        weights_init(self.conv2)\n",
        "        \n",
        "        kernel_size = 5\n",
        "        self.decode_conv1 = conv(1024, 512, kernel_size)\n",
        "        self.decode_conv2 = conv(768, 256, kernel_size)\n",
        "        self.decode_conv3 = conv(384, 128, kernel_size)\n",
        "        self.decode_conv4 = conv(192, 64, kernel_size)\n",
        "        self.decode_conv5 = conv(128, 32, kernel_size)\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # resnet\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x1 = self.relu(x)\n",
        "        # print(\"x1\", x1.size())\n",
        "        x2 = self.maxpool(x1)\n",
        "        # print(\"x2\", x2.size())\n",
        "        x3 = self.layer1(x2)\n",
        "        # print(\"x3\", x3.size())\n",
        "        x4 = self.layer2(x3)\n",
        "        # print(\"x4\", x4.size())\n",
        "        x5 = self.layer3(x4)\n",
        "        # print(\"x5\", x5.size())\n",
        "        x6 = self.layer4(x5)\n",
        "        # print(\"x6\", x6.size())\n",
        "        x7 = self.conv2(x6)\n",
        "\n",
        "        # decoder\n",
        "        y10 = self.decode_conv1(x7)\n",
        "        # print(\"y10\", y10.size())\n",
        "        y9 = F.interpolate(y10, scale_factor=2, mode='nearest')\n",
        "        # print(\"y9\", y9.size())\n",
        "        y8 = self.decode_conv2(torch.cat((y9, x5), 1))\n",
        "        # print(\"y8\", y8.size())\n",
        "        y7 = F.interpolate(y8, scale_factor=2, mode='nearest')\n",
        "        # print(\"y7\", y7.size())\n",
        "        y6 = self.decode_conv3(torch.cat((y7, x4), 1))\n",
        "        # print(\"y6\", y6.size())\n",
        "        y5 = F.interpolate(y6, scale_factor=2, mode='nearest')\n",
        "        # print(\"y5\", y5.size())\n",
        "        y4 = self.decode_conv4(torch.cat((y5, x3), 1))\n",
        "        # print(\"y4\", y4.size())\n",
        "        y3 = F.interpolate(y4, scale_factor=2, mode='nearest')\n",
        "        # print(\"y3\", y3.size())\n",
        "        y2 = self.decode_conv5(torch.cat((y3, x1), 1))\n",
        "        # print(\"y2\", y2.size())\n",
        "        y1 = F.interpolate(y2, scale_factor=2, mode='nearest')\n",
        "        # print(\"y1\", y1.size())\n",
        "        y = self.decode_conv6(y1)\n",
        "\n",
        "        return y\n",
        "\n",
        "class MobileNetSkipAdd(nn.Module):\n",
        "    def __init__(self, output_size, pretrained=True):\n",
        "\n",
        "        super(MobileNetSkipAdd, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        mobilenet = imagenet.mobilenet.MobileNet()\n",
        "        if pretrained:\n",
        "            pretrained_path = os.path.join('imagenet', 'results', 'imagenet.arch=mobilenet.lr=0.1.bs=256', 'model_best.pth.tar')\n",
        "            checkpoint = torch.load(pretrained_path)\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            from collections import OrderedDict\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:] # remove `module.`\n",
        "                new_state_dict[name] = v\n",
        "            mobilenet.load_state_dict(new_state_dict)\n",
        "        else:\n",
        "            mobilenet.apply(weights_init)\n",
        "\n",
        "        for i in range(14):\n",
        "            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n",
        "\n",
        "        kernel_size = 5\n",
        "        # self.decode_conv1 = conv(1024, 512, kernel_size)\n",
        "        # self.decode_conv2 = conv(512, 256, kernel_size)\n",
        "        # self.decode_conv3 = conv(256, 128, kernel_size)\n",
        "        # self.decode_conv4 = conv(128, 64, kernel_size)\n",
        "        # self.decode_conv5 = conv(64, 32, kernel_size)\n",
        "        self.decode_conv1 = nn.Sequential(\n",
        "            depthwise(1024, kernel_size),\n",
        "            pointwise(1024, 512))\n",
        "        self.decode_conv2 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 256))\n",
        "        self.decode_conv3 = nn.Sequential(\n",
        "            depthwise(256, kernel_size),\n",
        "            pointwise(256, 128))\n",
        "        self.decode_conv4 = nn.Sequential(\n",
        "            depthwise(128, kernel_size),\n",
        "            pointwise(128, 64))\n",
        "        self.decode_conv5 = nn.Sequential(\n",
        "            depthwise(64, kernel_size),\n",
        "            pointwise(64, 32))\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # skip connections: dec4: enc1\n",
        "        # dec 3: enc2 or enc3\n",
        "        # dec 2: enc4 or enc5\n",
        "        for i in range(14):\n",
        "            layer = getattr(self, 'conv{}'.format(i))\n",
        "            x = layer(x)\n",
        "            # print(\"{}: {}\".format(i, x.size()))\n",
        "            if i==1:\n",
        "                x1 = x\n",
        "            elif i==3:\n",
        "                x2 = x\n",
        "            elif i==5:\n",
        "                x3 = x\n",
        "        for i in range(1,6):\n",
        "            layer = getattr(self, 'decode_conv{}'.format(i))\n",
        "            x = layer(x)\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "            if i==4:\n",
        "                x = x + x1\n",
        "            elif i==3:\n",
        "                x = x + x2\n",
        "            elif i==2:\n",
        "                x = x + x3\n",
        "            # print(\"{}: {}\".format(i, x.size()))\n",
        "        x = self.decode_conv6(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileNetSkipConcat(nn.Module):\n",
        "    def __init__(self, output_size, pretrained=True):\n",
        "\n",
        "        super(MobileNetSkipConcat, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        mobilenet = MobileNet1()\n",
        "        if pretrained:\n",
        "            pretrained_path = os.path.join('imagenet', 'results', 'imagenet.arch=mobilenet.lr=0.1.bs=256', 'model_best.pth.tar')\n",
        "            checkpoint = torch.load(pretrained_path)\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            from collections import OrderedDict\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:] # remove `module.`\n",
        "                new_state_dict[name] = v\n",
        "            mobilenet.load_state_dict(new_state_dict)\n",
        "        else:\n",
        "            mobilenet.apply(weights_init)\n",
        "\n",
        "        for i in range(14):\n",
        "            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n",
        "\n",
        "        kernel_size = 5\n",
        "        # self.decode_conv1 = conv(1024, 512, kernel_size)\n",
        "        # self.decode_conv2 = conv(512, 256, kernel_size)\n",
        "        # self.decode_conv3 = conv(256, 128, kernel_size)\n",
        "        # self.decode_conv4 = conv(128, 64, kernel_size)\n",
        "        # self.decode_conv5 = conv(64, 32, kernel_size)\n",
        "        self.decode_conv1 = nn.Sequential(\n",
        "            depthwise(1024, kernel_size),\n",
        "            pointwise(1024, 512))\n",
        "        self.decode_conv2 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 256))\n",
        "        self.decode_conv3 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 128))\n",
        "        self.decode_conv4 = nn.Sequential(\n",
        "            depthwise(256, kernel_size),\n",
        "            pointwise(256, 64))\n",
        "        self.decode_conv5 = nn.Sequential(\n",
        "            depthwise(128, kernel_size),\n",
        "            pointwise(128, 32))\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # skip connections: dec4: enc1\n",
        "        # dec 3: enc2 or enc3\n",
        "        # dec 2: enc4 or enc5\n",
        "        for i in range(14):\n",
        "            layer = getattr(self, 'conv{}'.format(i))\n",
        "            x = layer(x)\n",
        "            print(\"{}: {}\".format(i, x.size()))\n",
        "            if i==1:\n",
        "                x1 = x\n",
        "            elif i==3:\n",
        "                x2 = x\n",
        "            elif i==5:\n",
        "                x3 = x\n",
        "        for i in range(1,6):\n",
        "            layer = getattr(self, 'decode_conv{}'.format(i))\n",
        "            print(\"{}a: {}\".format(i, x.size()))\n",
        "            x = layer(x)\n",
        "            print(\"{}b: {}\".format(i, x.size()))\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "            print(\"{}inter: {}\".format(i, x.size()))\n",
        "            if i==4:\n",
        "                x = torch.cat((x, x1), 1)\n",
        "            elif i==3:\n",
        "                x = torch.cat((x, x2), 1)\n",
        "            elif i==2:\n",
        "                x = torch.cat((x, x3), 1)\n",
        "            print(\"{}c: {}\".format(i, x.size()))\n",
        "        x = self.decode_conv6(x)\n",
        "        # print('END', x.size())\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydCWtC_sdWSs"
      },
      "source": [
        "### FastDepth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQMlZrJdZn6"
      },
      "source": [
        "class EfficientFastDepth(nn.Module):\n",
        "    def __init__(self, output_size, pretrained=True):\n",
        "\n",
        "        super(EfficientFastDepth, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        # mobilenet = MobileNet1()\n",
        "\n",
        "        # EFFICIENT NET #\n",
        "        efficientnet_name = 'efficientnet_lite0'\n",
        "        self.efficientnet = build_efficientnet_lite(efficientnet_name, 1000)\n",
        "\n",
        "        # Decoder\n",
        "        kernel_size = 5\n",
        "        self.decode_conv1 = nn.Sequential(\n",
        "            depthwise(1024, kernel_size),\n",
        "            pointwise(1024, 512))\n",
        "        self.decode_conv2 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 256))\n",
        "        self.decode_conv3 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 128))\n",
        "        self.decode_conv4 = nn.Sequential(\n",
        "            depthwise(256, kernel_size),\n",
        "            pointwise(256, 64))\n",
        "        self.decode_conv5 = nn.Sequential(\n",
        "            depthwise(128, kernel_size),\n",
        "            pointwise(128, 32))\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        x, x1, x2, x3 = self.efficientnet.forward(x)\n",
        "\n",
        "        # print('Encoder end: ', x.size())\n",
        "\n",
        "        for i in range(1,6):\n",
        "            layer = getattr(self, 'decode_conv{}'.format(i))\n",
        "            # print(\"{}a: {}\".format(i, x.size()))\n",
        "            x = layer(x)\n",
        "            # print(\"{}b: {}\".format(i, x.size()))\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest') # F.interpolate()\n",
        "            # print(\"{}inter: {}\".format(i, x.size()))\n",
        "\n",
        "            if i==4:\n",
        "                x = torch.cat((x, x1), 1)\n",
        "            elif i==3:\n",
        "                x = torch.cat((x, x2), 1)\n",
        "            elif i==2:\n",
        "                x = torch.cat((x, x3), 1)\n",
        "            # print(\"{}c: {}\".format(i, x.size()))\n",
        "            # print('---------')\n",
        "        x = self.decode_conv6(x)\n",
        "\n",
        "        # print('END', x.size())\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yrc6reUe_OG"
      },
      "source": [
        "class EfficientFastDepthOriginal(nn.Module):\n",
        "    def __init__(self, output_size, pretrained=True):\n",
        "\n",
        "        super(EfficientFastDepthOriginal, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        # mobilenet = MobileNet1()\n",
        "\n",
        "        # EFFICIENT NET #\n",
        "        efficientnet_name = 'efficientnet_lite0'\n",
        "        self.efficientnet = build_efficientnet_lite_original(efficientnet_name, 1000)\n",
        "\n",
        "        # Decoder\n",
        "        kernel_size = 5\n",
        "        self.decode_conv1 = nn.Sequential(\n",
        "            depthwise(1024, kernel_size),\n",
        "            pointwise(1024, 512))\n",
        "        self.decode_conv2 = nn.Sequential(\n",
        "            depthwise(512, kernel_size),\n",
        "            pointwise(512, 256))\n",
        "        self.decode_conv3 = nn.Sequential(\n",
        "            depthwise(256, kernel_size),\n",
        "            pointwise(256, 128))\n",
        "        self.decode_conv4 = nn.Sequential(\n",
        "            depthwise(128, kernel_size),\n",
        "            pointwise(128, 64))\n",
        "        self.decode_conv5 = nn.Sequential(\n",
        "            depthwise(64, kernel_size),\n",
        "            pointwise(64, 32))\n",
        "        self.decode_conv6 = pointwise(32, 1)\n",
        "        weights_init(self.decode_conv1)\n",
        "        weights_init(self.decode_conv2)\n",
        "        weights_init(self.decode_conv3)\n",
        "        weights_init(self.decode_conv4)\n",
        "        weights_init(self.decode_conv5)\n",
        "        weights_init(self.decode_conv6)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.efficientnet.forward(x)\n",
        "\n",
        "        for i in range(1,6):\n",
        "            layer = getattr(self, 'decode_conv{}'.format(i))\n",
        "\n",
        "            x = layer(x)\n",
        "\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest') # F.interpolate()\n",
        "\n",
        "        x = self.decode_conv6(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TOF20iGsAL"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZoV2oQ5GuGi"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def log10(x):\n",
        "      \"\"\"Convert a new tensor with the base-10 logarithm of the elements of x. \"\"\"\n",
        "      return torch.log(x) / math.log(10)\n",
        "\n",
        "class Result(object):\n",
        "    def __init__(self):\n",
        "        self.irmse, self.imae = 0, 0\n",
        "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
        "        self.absrel, self.lg10 = 0, 0\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "        self.data_time, self.gpu_time = 0, 0\n",
        "\n",
        "    def set_to_worst(self):\n",
        "        self.irmse, self.imae = np.inf, np.inf\n",
        "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
        "        self.absrel, self.lg10 = np.inf, np.inf\n",
        "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
        "        self.data_time, self.gpu_time = 0, 0\n",
        "\n",
        "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3, gpu_time, data_time):\n",
        "        self.irmse, self.imae = irmse, imae\n",
        "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
        "        self.absrel, self.lg10 = absrel, lg10\n",
        "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
        "        self.data_time, self.gpu_time = data_time, gpu_time\n",
        "\n",
        "    def evaluate(self, output, target):\n",
        "        valid_mask = ((target>0) + (output>0)) > 0\n",
        "\n",
        "        output = 1e3 * output[valid_mask]\n",
        "        target = 1e3 * target[valid_mask]\n",
        "        abs_diff = (output - target).abs()\n",
        "\n",
        "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
        "        self.rmse = math.sqrt(self.mse)\n",
        "        self.mae = float(abs_diff.mean())\n",
        "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
        "        self.absrel = float((abs_diff / target).mean())\n",
        "\n",
        "        maxRatio = torch.max(output / target, target / output)\n",
        "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
        "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
        "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
        "        self.data_time = 0\n",
        "        self.gpu_time = 0\n",
        "\n",
        "        inv_output = 1 / output\n",
        "        inv_target = 1 / target\n",
        "        abs_inv_diff = (inv_output - inv_target).abs()\n",
        "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
        "        self.imae = float(abs_inv_diff.mean())\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count = 0.0\n",
        "\n",
        "        self.sum_irmse, self.sum_imae = 0, 0\n",
        "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
        "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
        "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
        "        self.sum_data_time, self.sum_gpu_time = 0, 0\n",
        "\n",
        "    def update(self, result, gpu_time, data_time, n=1):\n",
        "        self.count += n\n",
        "\n",
        "        self.sum_irmse += n*result.irmse\n",
        "        self.sum_imae += n*result.imae\n",
        "        self.sum_mse += n*result.mse\n",
        "        self.sum_rmse += n*result.rmse\n",
        "        self.sum_mae += n*result.mae\n",
        "        self.sum_absrel += n*result.absrel\n",
        "        self.sum_lg10 += n*result.lg10\n",
        "        self.sum_delta1 += n*result.delta1\n",
        "        self.sum_delta2 += n*result.delta2\n",
        "        self.sum_delta3 += n*result.delta3\n",
        "        self.sum_data_time += n*data_time\n",
        "        self.sum_gpu_time += n*gpu_time\n",
        "\n",
        "    def average(self):\n",
        "        avg = Result()\n",
        "        avg.update(\n",
        "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
        "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
        "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
        "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count,\n",
        "            self.sum_gpu_time / self.count, self.sum_data_time / self.count)\n",
        "        return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6wz0jzvaKtI"
      },
      "source": [
        "### Argument Parser (Same as command line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RskB2WtsZ2TW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ece910a-9e2d-4fa2-c730-e09ad046bbc0"
      },
      "source": [
        "# args = parse_command()\n",
        "import argparse\n",
        "# argument parser instead of command line\n",
        "args = argparse.Namespace(arch='efficientnet', # mobilenet or efficientnet\n",
        "                          batch_size=8, # changed from 8\n",
        "                          criterion='BerHu', # changed from l1, l2, BerHu\n",
        "                          cuda=True, \n",
        "                          data='nyudepthv2', \n",
        "                          decoder='deconv2', \n",
        "                          epochs=15, \n",
        "                          evaluate='', \n",
        "                          gpu='0', \n",
        "                          lr=0.01, \n",
        "                          modality='rgb', \n",
        "                          momentum=0.9, \n",
        "                          num_samples=0, \n",
        "                          print_freq=50, \n",
        "                          train='ttt',\n",
        "                          weight_decay=0.0001, \n",
        "                          workers=16)\n",
        "\n",
        "print(args) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='efficientnet', batch_size=8, criterion='BerHu', cuda=True, data='nyudepthv2', decoder='deconv2', epochs=15, evaluate='', gpu='0', lr=0.01, modality='rgb', momentum=0.9, num_samples=0, print_freq=50, train='ttt', weight_decay=0.0001, workers=16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDCCb8OvLzkk"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4dLwhSP_6U"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "cudnn.benchmark = True\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class MaskedMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        assert pred.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "        valid_mask = (target>0).detach()\n",
        "        diff = target - pred\n",
        "        diff = diff[valid_mask]\n",
        "        self.loss = (diff ** 2).mean()\n",
        "        return self.loss\n",
        "\n",
        "class MaskedL1Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedL1Loss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        assert pred.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "        valid_mask = (target>0).detach()\n",
        "        diff = target - pred\n",
        "        diff = diff[valid_mask]\n",
        "        self.loss = diff.abs().mean()\n",
        "        return self.loss\n",
        "\n",
        "class MaskedHuberLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedHuberLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        assert pred.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "        valid_mask = (target>0).detach()\n",
        "        diff = torch.abs(target - pred)\n",
        "        diff = diff[valid_mask]\n",
        "        C = 0.2*torch.max(diff).item()\n",
        "        # self.loss = diff.abs().mean()\n",
        "        return torch.mean(torch.where(diff < C, diff,(diff*diff+C*C)/(2*C) ))\n",
        "\n",
        "class BerHuLoss(nn.Module):\n",
        "    \"\"\"Adds a Huber Loss term to the training procedure.\n",
        "    For each value x in `error=labels-predictions`, the following is calculated:\n",
        "    ```\n",
        "    |x|                        if |x| <= d\n",
        "    (x^2 + d^2)*0,5/d  if |x| > d\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BerHuLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):  # delta changes everytime - per batch\n",
        "        # all variables here must be torch.autograd.Variable to perform autograd\n",
        "        assert input.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "        absError = torch.abs(target - input)\n",
        "        delta = 0.2 * torch.max(absError).item()\n",
        "\n",
        "        L2 = (absError * absError / delta + delta) * 0.5\n",
        "\n",
        "        mask_down_f = absError.le(delta).float()\n",
        "        mask_up_f = absError.gt(delta).float()\n",
        "\n",
        "        loss = absError * mask_down_f + L2 * mask_up_f\n",
        "\n",
        "        return torch.mean(loss)\n",
        "\n",
        "class EigenLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EigenLoss, self).__init__()\n",
        "\n",
        "    def _data_in_log_meters(self, data):\n",
        "        \"\"\" log((0.5 * (data + 1)) * 10) in meters\"\"\"\n",
        "        # data is between [-1, 1]\n",
        "        # we are going to apply log, but according to the original distance\n",
        "        return torch.log(5 * (data + 1))\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        assert input.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "        # get data in log meters\n",
        "        log_input, log_target = self._data_in_log_meters(input), self._data_in_log_meters(target)\n",
        "\n",
        "        # number of elements\n",
        "        n_el = log_input.data.numel()\n",
        "        error = log_input - log_target\n",
        "\n",
        "        # L2 loss\n",
        "        loss1 = torch.mean(error * error)\n",
        "\n",
        "        # scale invariant difference\n",
        "        mean_error = torch.mean(error)\n",
        "        loss2 = (mean_error * mean_error) * 0.5 / (n_el * n_el)\n",
        "\n",
        "        loss = loss1 - loss2\n",
        "\n",
        "        return loss\n",
        "\n",
        "        \n",
        "class EigenGradLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    d = log(input) - log(target)\n",
        "    L()  = (mean(d^2)) - lambda / n^2 * (mean(d)^2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EigenGradLoss, self).__init__()\n",
        "        self._lambda = 0.5   # like in Eigen's 2014 paper\n",
        "        # self.mask = Variable(torch.FloatTensor(opt.batchSize, 1, opt.imageSize[0], opt.imageSize[1]), requires_grad=False).cuda()\n",
        "        self.mask = Variable(torch.FloatTensor(8, 1, 224, 224), requires_grad=False).cuda()\n",
        "\n",
        "    def _data_in_log_meters(self, data):\n",
        "        \"\"\" log((0.5 * (data + 1)) * 10) in meters\"\"\"\n",
        "        return torch.log(5 * (data + 1))\n",
        "\n",
        "    def _sobel_window_x(self):\n",
        "        return torch.Tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
        "\n",
        "    def _sobel_window_y(self):\n",
        "        return -self._sobel_window_x().transpose(0, 1)\n",
        "\n",
        "    def get_mask_invalid_pixels(self, target, value=-0.9):\n",
        "        mask_ByteTensor = (target.data > value)\n",
        "        with torch.no_grad():\n",
        "          self.mask.resize_(mask_ByteTensor.size()).copy_(mask_ByteTensor)\n",
        "        # self.outG = self.outG * self.mask\n",
        "        # self.target = self.target * self.mask\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        assert input.dim() == target.dim(), \"inconsistent dimensions\"\n",
        "\n",
        "        self.get_mask_invalid_pixels(target)    # modifies self.mask variable\n",
        "\n",
        "        # get data in log meters\n",
        "        log_input = self._data_in_log_meters(input * self.mask)\n",
        "        log_target = self._data_in_log_meters(target * self.mask)\n",
        "\n",
        "        # number of elements\n",
        "        error_image = (log_input - log_target)\n",
        "        n_el = error_image.data.numel()\n",
        "\n",
        "        # L2 loss\n",
        "        loss1 = torch.mean(error_image * error_image)\n",
        "\n",
        "        # scale invariant difference\n",
        "        mean_error = torch.mean(error_image)\n",
        "        loss2 = (mean_error * mean_error) * 0.5 / (n_el * n_el)\n",
        "\n",
        "        # horizontal and vertical gradients\n",
        "        # apply sobel filter\n",
        "        _filter_x = Variable(self._sobel_window_x().unsqueeze(0).unsqueeze(0).cuda())\n",
        "        _filter_y = Variable(self._sobel_window_y().unsqueeze(0).unsqueeze(0).cuda())\n",
        "\n",
        "        grad_x = F.conv2d(error_image, _filter_x, padding=1) * self.mask  # use conv2 with predefined weights\n",
        "        grad_y = F.conv2d(error_image, _filter_y, padding=1) * self.mask\n",
        "        loss3 = torch.mean(grad_x * grad_x + grad_y * grad_y)\n",
        "\n",
        "        loss = loss1 - loss2 + loss3\n",
        "\n",
        "        return loss\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu # Set the GPU.\n",
        "\n",
        "fieldnames = ['rmse', 'mae', 'delta1', 'absrel',\n",
        "            'lg10', 'mse', 'delta2', 'delta3', 'data_time', 'gpu_time']\n",
        "best_fieldnames = ['best_epoch'] + fieldnames\n",
        "best_result = Result()\n",
        "best_result.set_to_worst()\n",
        "\n",
        "\n",
        "class DenseToSparse:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def dense_to_sparse(self, rgb, depth):\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class UniformSampling(DenseToSparse):\n",
        "    name = \"uar\"\n",
        "    def __init__(self, num_samples, max_depth=np.inf):\n",
        "        DenseToSparse.__init__(self)\n",
        "        self.num_samples = num_samples\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"%s{ns=%d,md=%f}\" % (self.name, self.num_samples, self.max_depth)\n",
        "\n",
        "    def dense_to_sparse(self, rgb, depth):\n",
        "        \"\"\"\n",
        "        Samples pixels with `num_samples`/#pixels probability in `depth`.\n",
        "        Only pixels with a maximum depth of `max_depth` are considered.\n",
        "        If no `max_depth` is given, samples in all pixels\n",
        "        \"\"\"\n",
        "        mask_keep = depth > 0\n",
        "        if self.max_depth is not np.inf:\n",
        "            mask_keep = np.bitwise_and(mask_keep, depth <= self.max_depth)\n",
        "        n_keep = np.count_nonzero(mask_keep)\n",
        "        if n_keep == 0:\n",
        "            return mask_keep\n",
        "        else:\n",
        "            prob = float(self.num_samples) / n_keep\n",
        "            return np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)\n",
        "\n",
        "\n",
        "def create_data_loaders(args):\n",
        "    # Data loading code\n",
        "    print(\"=> creating data loaders ...\")\n",
        "    traindir = '/content/drive/MyDrive/nyudepthv2/train'\n",
        "    valdir = '/content/drive/MyDrive/nyudepthv2/val'\n",
        "\n",
        "    print(\"PATHS\")\n",
        "    print(\"train path: \" , traindir)\n",
        "    print(\"val path: \" , valdir)\n",
        "    train_loader = None\n",
        "    val_loader = None\n",
        "    dataset = 'nyudepthv2'\n",
        "\n",
        "\n",
        "    # sparsifier is a class for generating random sparse depth input from the ground truth\n",
        "    sparsifier = None\n",
        "    max_depth = 0.0 # if max_depth >= 0.0 else np.inf\n",
        "    sparsifier = UniformSampling(num_samples=args.num_samples, max_depth=max_depth)\n",
        "\n",
        "    # if args.sparsifier == UniformSampling.name:\n",
        "    #     sparsifier = UniformSampling(num_samples=args.num_samples, max_depth=max_depth)\n",
        "    # elif args.sparsifier == SimulatedStereo.name:\n",
        "    #     sparsifier = SimulatedStereo(num_samples=args.num_samples, max_depth=max_depth)\n",
        "\n",
        "    if dataset == 'nyudepthv2':\n",
        "        \n",
        "        if not args.evaluate:\n",
        "            train_dataset = NYUDataset(traindir, split='train',\n",
        "                modality='rgb')\n",
        "        val_dataset = NYUDataset(valdir, split='val',\n",
        "            modality='rgb')\n",
        "\n",
        "\n",
        "    # set batch size to be 1 for validation\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "        batch_size=1, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "    print(\"len val loader inside funct: \" , len(val_loader))\n",
        "    # put construction of train loader here, for those who are interested in testing only\n",
        "    if not args.evaluate:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "            num_workers=args.workers, pin_memory=True, sampler=None,\n",
        "            worker_init_fn=lambda work_id:np.random.seed(work_id))\n",
        "            # worker_init_fn ensures different sampling patterns for each data loading thread\n",
        "\n",
        "    print(\"=> data loaders created.\")\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE43on7zah0d"
      },
      "source": [
        "# Run Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPD4zxidQJsk"
      },
      "source": [
        "## main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZg8_z_HwMe"
      },
      "source": [
        "def main():\n",
        "    global args, best_result, output_directory, train_csv, test_csv, output_directory1, train_csv1, test_csv1\n",
        "\n",
        "    # Data loading code\n",
        "    print(\"=> creating data loaders...\")\n",
        "    # valdir = os.path.join('..', 'data', args.data, 'val')\n",
        "\n",
        "    # if args.data == 'nyudepthv2':\n",
        "        \n",
        "    #     val_dataset = NYUDataset(valdir, split='val', modality=args.modality)\n",
        "    # else:\n",
        "    #     raise RuntimeError('Dataset not found.')\n",
        "\n",
        "    # set batch size to be 1 for validation\n",
        "    # val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "    #     batch_size=1, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "    # print(\"=> data loaders created.\")\n",
        "\n",
        "    # evaluation mode\n",
        "    if args.evaluate:\n",
        "        assert os.path.isfile(args.evaluate), \\\n",
        "        \"=> no model found at '{}'\".format(args.evaluate)\n",
        "        print(\"=> loading model '{}'\".format(args.evaluate))\n",
        "        checkpoint = torch.load(args.evaluate)\n",
        "        if type(checkpoint) is dict:\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_result = checkpoint['best_result']\n",
        "            model = checkpoint['model']\n",
        "            print(\"=> loaded best model (epoch {})\".format(checkpoint['epoch']))\n",
        "        else:\n",
        "            model = checkpoint\n",
        "            args.start_epoch = 0\n",
        "        output_directory = os.path.dirname(args.evaluate)\n",
        "        validate(val_loader, model, args.start_epoch, write_to_file=False)\n",
        "    elif args.train:\n",
        "        modality = 'rgb'\n",
        "        start_epoch = 0\n",
        "        lr = 0.01\n",
        "        momentum = 0.9\n",
        "        weight_decay = 1e-4 \n",
        "        train_loader, val_loader = create_data_loaders(args)\n",
        "        print(\"len train : \" , len(train_loader))\n",
        "        print(\"len val : \" , len(val_loader))\n",
        "        print(\"=> creating Model ({}-{}) ...\".format(args.arch, args.decoder))\n",
        "        in_channels = len(modality)\n",
        "        if args.arch == 'resnet50':\n",
        "            model = ResNet(layers=50, decoder=args.decoder, output_size=train_loader.dataset.output_size,\n",
        "                in_channels=in_channels, pretrained=False)\n",
        "        elif args.arch == 'resnet18':\n",
        "            model = ResNet(layers=18, decoder=args.decoder, output_size=train_loader.dataset.output_size,\n",
        "                in_channels=in_channels, pretrained=False)\n",
        "            \n",
        "        # - - - - - - MobileNet - - - - - - # [default]\n",
        "        elif args.arch == 'mobilenet':\n",
        "            model = MobileNetSkipConcat(output_size=train_loader.dataset.output_size, pretrained=False)\n",
        "\n",
        "                # - - - - - EfficientNets - - - - - #\n",
        "        elif 'efficientnetOriginal' in args.arch:\n",
        "            model = EfficientFastDepthOriginal(output_size=train_loader.dataset.output_size, pretrained=False)\n",
        "\n",
        "        # - - - - - EfficientNets - - - - - # \n",
        "        elif 'efficientnet' in args.arch:\n",
        "            model = EfficientFastDepth(output_size=train_loader.dataset.output_size, pretrained=False)\n",
        "\n",
        "        # - - - - - - - - - - - - - - - - - # \n",
        "\n",
        "        print(\"=> model created.\")\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr, \\\n",
        "            momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "        # model = torch.nn.DataParallel(model).cuda() # for multi-gpu training\n",
        "        model = model.cuda()\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    if args.criterion == 'l2':\n",
        "        print(\"CHOSEN LOSS FUNCTION: L2\")\n",
        "        criterion = MaskedMSELoss().cuda()\n",
        "    elif args.criterion == 'l1':\n",
        "        print(\"CHOSEN LOSS FUNCTION: L1\")\n",
        "        criterion = MaskedL1Loss().cuda()\n",
        "    elif args.criterion == 'BerHu':\n",
        "        print(\"CHOSEN LOSS FUNCTION: BerHu\")\n",
        "        criterion = BerHuLoss().cuda()\n",
        "    elif args.criterion == 'MiDas':\n",
        "        print(\"CHOSEN LOSS FUNCTION: MiDas\")\n",
        "        criterion = ScaleAndShiftInvariantLoss().cuda()\n",
        "    elif args.criterion == 'EigenLoss':\n",
        "        print(\"CHOSEN LOSS FUNCTION: EigenLoss\")\n",
        "        criterion = EigenLoss().cuda()\n",
        "  \n",
        "\n",
        "    # create results folder, if not already exists\n",
        "    output_directory = os.path.join('results',\n",
        "        'Dataset={} -- Network={} -- Loss={} -- lr={} -- bs={}'.\n",
        "        format(args.data, \\\n",
        "            args.arch, args.criterion, args.lr, args.batch_size))\n",
        "    output_directory1 = os.path.join('/content/drive/MyDrive/results',\n",
        "        'Dataset={} -- Network={} -- Loss={} -- lr={} -- bs={}'.\n",
        "        format(args.data, \\\n",
        "            args.arch, args.criterion, args.lr, args.batch_size))\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "    if not os.path.exists(output_directory1):\n",
        "        os.makedirs(output_directory1)\n",
        "    train_csv = os.path.join(output_directory, 'train.csv')\n",
        "    test_csv = os.path.join(output_directory, 'test.csv')\n",
        "    best_txt = os.path.join(output_directory, 'best.txt')\n",
        "    train_csv1 = os.path.join(output_directory1, 'train.csv')\n",
        "    test_csv1 = os.path.join(output_directory1, 'test.csv')\n",
        "    best_txt1 = os.path.join(output_directory1, 'best.txt')\n",
        "    print('created result folders.. ')\n",
        "\n",
        "    # create new csv files with only header\n",
        "    if args.train or args.evaluate:\n",
        "        with open(train_csv, 'w') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "        with open(test_csv, 'w') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "        with open(train_csv1, 'w') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "        with open(test_csv1, 'w') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "    print('created csv files.. ')\n",
        "\n",
        "    print('-- STARTING TRAINING -- ')\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        adjust_learning_rate(optimizer, epoch, args.lr)\n",
        "        train(train_loader, model, criterion, optimizer, epoch) # train for one epoch\n",
        "        # print('trained')\n",
        "        result, img_merge = validate(val_loader, model, epoch) # evaluate on validation set\n",
        "        # print('validated')\n",
        "        # remember best rmse and save checkpoint\n",
        "        is_best = result.rmse < best_result.rmse\n",
        "        if is_best:\n",
        "            best_result = result\n",
        "            with open(best_txt, 'w') as txtfile:\n",
        "                txtfile.write(\"epoch={}\\nmse={:.3f}\\nrmse={:.3f}\\nabsrel={:.3f}\\nlg10={:.3f}\\nmae={:.3f}\\ndelta1={:.3f}\\nt_gpu={:.4f}\\n\".\n",
        "                    format(epoch, result.mse, result.rmse, result.absrel, result.lg10, result.mae, result.delta1, result.gpu_time))\n",
        "            with open(best_txt1, 'w') as txtfile:\n",
        "                txtfile.write(\"epoch={}\\nmse={:.3f}\\nrmse={:.3f}\\nabsrel={:.3f}\\nlg10={:.3f}\\nmae={:.3f}\\ndelta1={:.3f}\\nt_gpu={:.4f}\\n\".\n",
        "                    format(epoch, result.mse, result.rmse, result.absrel, result.lg10, result.mae, result.delta1, result.gpu_time))\n",
        "            if img_merge is not None:\n",
        "                img_filename = output_directory + '/comparison_best.png'\n",
        "                save_image(img_merge, img_filename)\n",
        "                img_filename1 = output_directory1 + '/comparison_best.png'\n",
        "                save_image(img_merge, img_filename1)\n",
        "\n",
        "        save_checkpoint({\n",
        "            'args': args,\n",
        "            'epoch': epoch,\n",
        "            'arch': args.arch,\n",
        "            'model': model,\n",
        "            'best_result': best_result,\n",
        "            'optimizer' : optimizer,\n",
        "        }, is_best, epoch, output_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTNPm8JGQjOf"
      },
      "source": [
        "## train()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wzx_JL1QhgK"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    print('train called')\n",
        "    average_meter = AverageMeter()\n",
        "    model.train() # switch to train mode\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        data_time = time.time() - end\n",
        "\n",
        "        # compute pred\n",
        "        end = time.time()\n",
        "\n",
        "        # print('Calling model for prediction.')\n",
        "        pred = model(input)\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "        # print('creating masks.. ')\n",
        "        # mask  = target > 0\n",
        "        # target[mask] = (target[mask] - target[mask].min()) / (target[mask].max() - target[mask].min()) * 9 + 1\n",
        "        # target[mask] = 10. / target[mask]\n",
        "        # target[~mask] = 0.\n",
        "\n",
        "        # print('calling loss')\n",
        "        loss = criterion(pred, target)\n",
        "        ###############################################\n",
        "\n",
        "        # print('calling optimizer')\n",
        "        optimizer.zero_grad()\n",
        "        # print('backprop')\n",
        "        loss.backward() # compute gradient and do SGD step\n",
        "        optimizer.step()\n",
        "        torch.cuda.synchronize()\n",
        "        gpu_time = time.time() - end\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        result = Result()\n",
        "        result.evaluate(pred.data, target.data)\n",
        "        average_meter.update(result, gpu_time, data_time, input.size(0))\n",
        "        end = time.time()\n",
        "\n",
        "        if (i + 1) % args.print_freq == 0:\n",
        "            print('=> output: {}'.format(output_directory))\n",
        "            print('Train Epoch: {0} [{1}/{2}]\\t'\n",
        "                  't_Data={data_time:.3f}({average.data_time:.3f}) '\n",
        "                  't_GPU={gpu_time:.3f}({average.gpu_time:.3f})\\n\\t'\n",
        "                  'RMSE={result.rmse:.2f}({average.rmse:.2f}) '\n",
        "                  'MAE={result.mae:.2f}({average.mae:.2f}) '\n",
        "                  'Delta1={result.delta1:.3f}({average.delta1:.3f}) '\n",
        "                  'REL={result.absrel:.3f}({average.absrel:.3f}) '\n",
        "                  'Lg10={result.lg10:.3f}({average.lg10:.3f}) '.format(\n",
        "                  epoch, i+1, len(train_loader), data_time=data_time,\n",
        "                  gpu_time=gpu_time, result=result, average=average_meter.average()))\n",
        "\n",
        "    avg = average_meter.average()\n",
        " \n",
        "    with open(train_csv, 'a') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "            'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3,\n",
        "            'gpu_time': avg.gpu_time, 'data_time': avg.data_time})\n",
        "        \n",
        "    with open(train_csv1, 'a') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "            'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3,\n",
        "            'gpu_time': avg.gpu_time, 'data_time': avg.data_time})\n",
        "        \n",
        "    \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-RrIyJ5QTFm"
      },
      "source": [
        "## validate()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgPH3W0qQQ5-"
      },
      "source": [
        "def validate(val_loader, model, epoch, write_to_file=True):\n",
        "    average_meter = AverageMeter()\n",
        "    model.eval() # switch to evaluate mode\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "        # torch.cuda.synchronize()\n",
        "        data_time = time.time() - end\n",
        "\n",
        "        # compute output\n",
        "        end = time.time()\n",
        "        with torch.no_grad():\n",
        "            pred = model(input)\n",
        "        # torch.cuda.synchronize()\n",
        "        gpu_time = time.time() - end\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        result = Result()\n",
        "        result.evaluate(pred.data, target.data)\n",
        "        average_meter.update(result, gpu_time, data_time, input.size(0))\n",
        "        end = time.time()\n",
        "\n",
        "        # save 8 images for visualization\n",
        "        skip = 50\n",
        "\n",
        "        if args.modality == 'rgb':\n",
        "            rgb = input\n",
        "\n",
        "        if i == 0:\n",
        "            img_merge = merge_into_row(rgb, target, pred)\n",
        "        elif (i < 8*skip) and (i % skip == 0):\n",
        "            row = merge_into_row(rgb, target, pred)\n",
        "            img_merge = add_row(img_merge, row)\n",
        "        elif i == 8*skip:\n",
        "            filename = output_directory + '/comparison_' + str(epoch) + '.png'\n",
        "            save_image(img_merge, filename)\n",
        "            filename1 = output_directory1 + '/comparison_' + str(epoch) + '.png'\n",
        "            save_image(img_merge, filename1)\n",
        "\n",
        "        if (i+1) % args.print_freq == 0:\n",
        "            print('Test: [{0}/{1}]\\t'\n",
        "                  't_GPU={gpu_time:.3f}({average.gpu_time:.3f})\\n\\t'\n",
        "                  'RMSE={result.rmse:.2f}({average.rmse:.2f}) '\n",
        "                  'MAE={result.mae:.2f}({average.mae:.2f}) '\n",
        "                  'Delta1={result.delta1:.3f}({average.delta1:.3f}) '\n",
        "                  'REL={result.absrel:.3f}({average.absrel:.3f}) '\n",
        "                  'Lg10={result.lg10:.3f}({average.lg10:.3f}) '.format(\n",
        "                   i+1, len(val_loader), gpu_time=gpu_time, result=result, average=average_meter.average()))\n",
        "\n",
        "    avg = average_meter.average()\n",
        "\n",
        "    print('\\n*\\n'\n",
        "        'RMSE={average.rmse:.3f}\\n'\n",
        "        'MAE={average.mae:.3f}\\n'\n",
        "        'Delta1={average.delta1:.3f}\\n'\n",
        "        'REL={average.absrel:.3f}\\n'\n",
        "        'Lg10={average.lg10:.3f}\\n'\n",
        "        't_GPU={time:.3f}\\n'.format(\n",
        "        average=avg, time=avg.gpu_time))\n",
        "\n",
        "    if write_to_file:\n",
        "        with open(test_csv, 'a') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "                'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3,\n",
        "                'data_time': avg.data_time, 'gpu_time': avg.gpu_time})\n",
        "        with open(test_csv1, 'a') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
        "                'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3,\n",
        "                'data_time': avg.data_time, 'gpu_time': avg.gpu_time})\n",
        "    return avg, img_merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOLbBze-WFbk"
      },
      "source": [
        "main()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Ivu0TqnQ3T"
      },
      "source": [
        "# Size Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSl8leLkVeIp"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "  table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "  total_params = 0\n",
        "  for name, parameter in model.named_parameters():\n",
        "      if not parameter.requires_grad: continue\n",
        "      param = parameter.numel()\n",
        "      table.add_row([name, param])\n",
        "      total_params+=param\n",
        "  print(table)\n",
        "  print(f\"Total Trainable Params: {total_params}\")\n",
        "  return total_params\n",
        "\n",
        "# count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju9339wt77jg"
      },
      "source": [
        "model = EfficientFastDepth(output_size=(3,224,224), pretrained=False) #6.452.810 params\n",
        "# model = EfficientFastDepthOriginal((3,224,224), pretrained=False) #5.067.530 params\n",
        "# model = MobileNetSkipConcat(output_size=(3,224,224), pretrained=False) #4.016.034 params\n",
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}